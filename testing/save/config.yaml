config_version: 1.0
training:
  trainer: mmf
  seed: 49822351
  experiment_name: run
  max_updates: 22000
  max_epochs: null
  log_interval: 100
  logger_level: info
  log_format: simple
  log_detailed_config: false
  should_not_log: false
  stdout_capture: true
  tensorboard: false
  batch_size: 32
  num_workers: 4
  fast_read: false
  dataset_size_proportional_sampling: true
  pin_memory: false
  checkpoint_interval: 1000
  evaluation_interval: 1000
  clip_gradients: false
  clip_norm_mode: all
  early_stop:
    enabled: false
    patience: 4000
    criteria: hateful_memes/roc_auc
    minimize: false
  lr_scheduler: true
  lr_steps: []
  lr_ratio: 0.1
  use_warmup: false
  warmup_factor: 0.2
  warmup_iterations: 1000
  device: cpu
  local_rank: null
  verbose_dump: false
  find_unused_parameters: true
  evaluate_metrics: false
evaluation:
  metrics:
  - accuracy
  - binary_f1
  - roc_auc
  predict: 'true'
  predict_file_format: json
model_config:
  vilbert:
    bert_model_name: bert-base-uncased
    training_head_type: classification
    visual_embedding_dim: 2048
    special_visual_initialize: true
    hard_cap_seq_len: null
    cut_first: text
    embedding_strategy: plain
    bypass_transformer: false
    output_attentions: false
    output_hidden_states: false
    text_only: false
    random_initialize: false
    freeze_base: false
    finetune_lr_multiplier: 1
    attention_probs_dropout_prob: 0.1
    layer_norm_eps: 1.0e-12
    hidden_act: gelu
    hidden_dropout_prob: 0.1
    hidden_size: 768
    initializer_range: 0.02
    intermediate_size: 3072
    max_position_embeddings: 512
    num_attention_heads: 12
    num_hidden_layers: 12
    type_vocab_size: 2
    vocab_size: 30522
    v_feature_size: 2048
    v_target_size: 1601
    v_hidden_size: 1024
    v_num_hidden_layers: 6
    v_num_attention_heads: 8
    v_intermediate_size: 1024
    bi_hidden_size: 1024
    bi_num_attention_heads: 8
    bi_intermediate_size: 1024
    bi_attention_type: 1
    v_attention_probs_dropout_prob: 0.1
    v_hidden_act: gelu
    v_hidden_dropout_prob: 0.1
    v_initializer_range: 0.02
    v_biattention_id:
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
    t_biattention_id:
    - 6
    - 7
    - 8
    - 9
    - 10
    - 11
    pooling_method: mul
    fusion_method: mul
    fast_mode: false
    with_coattention: true
    dynamic_attention: false
    in_batch_pairs: false
    task_specific_tokens: false
    fixed_v_layer: 0
    fixed_t_layer: 0
    visualization: false
    visual_target: 0
    objective: 0
    num_negative: 128
    model: vilbert
    num_labels: 2
    losses:
    - cross_entropy
dataset_config:
  hateful_memes:
    use_images: false
    use_features: true
    return_features_info: true
    processors:
      text_processor:
        type: bert_tokenizer
        params:
          tokenizer_config:
            type: bert-base-uncased
            params:
              do_lower_case: true
          mask_probability: 0
          max_seq_length: 128
datasets: hateful_memes
model: vilbert
config: projects/hateful_memes/configs/vilbert/from_cc.yaml
run_type: val
optimizer:
  type: adam_w
  params:
    lr: 1.0e-05
    eps: 1.0e-08
scheduler:
  type: warmup_linear
  params:
    num_warmup_steps: 2000
    num_training_steps: 22000
env:
  cache_dir: /Users/ahemf/.cache/torch/mmf
  dataset_zoo: configs/zoo/datasets.yaml
  model_zoo: configs/zoo/models.yaml
  data_dir: /Users/ahemf/.cache/torch/mmf/data
  save_dir: ./save
  log_dir: ''
  report_dir: ''
  tensorboard_logdir: ''
  user_dir: ''
distributed:
  init_method: null
  rank: 0
  port: -1
  backend: nccl
  world_size: 1
  no_spawn: false
checkpoint:
  resume: false
  resume_file: null
  resume_best: false
  resume_pretrained: true
  resume_zoo: vilbert.finetuned.hateful_memes.from_cc_original
  zoo_config_override: false
  pretrained_state_mapping:
    model.bert: model.bert
  max_to_keep: -1
  save_git_details: true
  reset:
    all: false
    optimizer: false
    counts: false
start_rank: 0
device_id: 0

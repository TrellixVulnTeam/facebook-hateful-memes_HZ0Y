{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T16:44:26.106267Z",
     "start_time": "2020-08-01T16:44:22.825629Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "import random\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "set_global(\"dataloader_workers\", 4)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, random_word_mask, my_collate\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T16:44:26.110506Z",
     "start_time": "2020-08-01T16:44:26.108052Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = get_datasets(data_dir=\"../data/\", train_text_transform=None, train_image_transform=None, \n",
    "#                     test_text_transform=None, test_image_transform=None, \n",
    "#                     cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "#                     keep_original_text=False, keep_original_image=False, \n",
    "#                     keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "# texts = list(data[\"train\"].text.values) + list(data[\"test\"].text.values) + list(data[\"dev\"].text.values)\n",
    "# pd.DataFrame({\"texts\": texts}).to_csv(\"text.csv\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T16:44:28.058442Z",
     "start_time": "2020-08-01T16:44:26.112152Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_model = \"/home/ahemf/cache/distilbert-nsp\"\n",
    "model_type = \"distilroberta-base\"\n",
    "output_model = \"/home/ahemf/cache/distilbert\"\n",
    "batch_size = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.vocab_size\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "tokenizer.vocab_size\n",
    "tokenizer.sep_token\n",
    "tokenizer.sep_token_id\n",
    "tokenizer.special_tokens_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "texts = pd.read_csv(\"text.csv\", header=None)[0].values\n",
    "\n",
    "def save(model, tokenizer, output_dir):\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Augmented/Masked Text same as Original Text - RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:13:44.086637Z",
     "start_time": "2020-08-01T12:10:23.517028Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class BertVectorizer(torch.nn.Module):\n",
    "    def __init__(self, model, sequence_length, pooled_multiplier):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.model = AutoModel.from_pretrained(model)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.pooled_multiplier = pooled_multiplier\n",
    "\n",
    "    def forward(self, texts):\n",
    "        tokenizer = self.tokenizer\n",
    "        n_tokens_in = self.sequence_length\n",
    "        converted_texts = tokenizer.batch_encode_plus(texts,\n",
    "                                                      add_special_tokens=True,\n",
    "                                                      pad_to_max_length=True,\n",
    "                                                      max_length=n_tokens_in,\n",
    "                                                      truncation=True)\n",
    "        input_ids, attention_mask = converted_texts[\n",
    "            \"input_ids\"], converted_texts[\"attention_mask\"]\n",
    "        input_ids, attention_mask = torch.tensor(input_ids).to(\n",
    "            get_device()), torch.tensor(attention_mask).to(get_device())\n",
    "        outputs = self.model(input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_states = outputs[0].mean(1).squeeze()\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = pooled_output * self.pooled_multiplier\n",
    "        output = torch.cat((pooled_output, last_hidden_states), 1)\n",
    "        return output\n",
    "\n",
    "\n",
    "from torchtext.data import Dataset, TabularDataset, Field\n",
    "dataset = Dataset(texts, [(\"text\", Field())])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"finetune\": True\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "model_fn = model_builder(\n",
    "    BertVectorizer,\n",
    "    dict(sequence_length=96, pooled_multiplier=4,\n",
    "         model=initial_model),  # initial_model\n",
    "    per_param_opts_fn=lr_strategy,\n",
    "    optimiser_class=optimizer,\n",
    "    optimiser_params=optimizer_params)\n",
    "\n",
    "model, optim = model_fn()\n",
    "\n",
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.65, 0.25], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"word_cutout\": 0.8, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.65, 0.25], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.3, \"half_cut\":0.0, \"part_select\": 0.75}\n",
    "    sentence_level = TextAugment([0.55, 0.45], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "vectorized_text_processor = np.vectorize(preprocess_text)\n",
    "\n",
    "\n",
    "def augment_method(texts):\n",
    "    mask_proba = 0.15\n",
    "    texts = [random_word_mask(t, model.tokenizer, mask_proba) for t in texts]\n",
    "    return vectorized_text_processor(texts)\n",
    "\n",
    "\n",
    "_ = train_for_augment_similarity(model,\n",
    "                                 optim,\n",
    "                                 scheduler_init_fn,\n",
    "                                 batch_size,\n",
    "                                 epochs,\n",
    "                                 dataset,\n",
    "                                 augment_method=augment_method,\n",
    "                                 model_call_back=None,\n",
    "                                 accumulation_steps=2,\n",
    "                                 plot=True)\n",
    "\n",
    "# 0.06687\n",
    "\n",
    "save(model.model, model.tokenizer, output_dir=output_model + \"-augsim\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence [Not useful]\n",
    "- Two halves belong to same sentence \n",
    "- Is first half swapped with 2nd half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.9, 0.], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"word_cutout\": 0.8, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.9, 0.], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.2, \"half_cut\":0.0, \"part_select\": 0.8}\n",
    "    sentence_level = TextAugment([0.75, 0.25], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "def split(text):\n",
    "    parts = text.split()\n",
    "    half = len(parts) // 2\n",
    "    return \" \".join(parts[:half]), \" \".join(parts[half:])\n",
    "class CoherenceDataset(Dataset):\n",
    "    def __init__(self, texts, preprocess_text, separator_token):\n",
    "        self.texts = list(texts)\n",
    "        self.sep = separator_token\n",
    "        self.preprocess_text = preprocess_text\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        org_text = self.texts[item]\n",
    "        text = self.preprocess_text(org_text)\n",
    "        text_part_1, text_part_2 = split(text)\n",
    "        if random.random() < 0.5:\n",
    "            if random.random() < 0.5:\n",
    "                return Sample({\"text\": self.__build_split__(text_part_1, text_part_2), \"label\": 1})\n",
    "            else:\n",
    "                return Sample({\"text\": self.__build_split__(text_part_2, text_part_1), \"label\": 0})\n",
    "        else:\n",
    "            text2 = self.preprocess_text(random.sample(self.texts, 1)[0])\n",
    "            text2_part_1, text2_part_2 = split(text)\n",
    "            if random.random() < 0.5:\n",
    "                if random.random() < 0.5:\n",
    "                    return Sample({\"text\": self.__build_split__(text_part_1, text2_part_2), \"label\": 0})\n",
    "                else:\n",
    "                    return Sample({\"text\": self.__build_split__(text2_part_1, text_part_2), \"label\": 0})\n",
    "            else:\n",
    "                if random.random() < 0.6:\n",
    "                    return Sample({\"text\": self.__build_split__(text, text2), \"label\": 0})\n",
    "                else:\n",
    "                    if random.random() < 0.5:\n",
    "                        return Sample({\"text\": self.__build_split__(org_text, text), \"label\": 1})\n",
    "                    else:\n",
    "                        return Sample({\"text\": self.__build_split__(text, org_text), \"label\": 1})\n",
    "    \n",
    "    def __build_split__(self, text_part_1, text_part_2):\n",
    "        if random.random() < 0.5:\n",
    "            return text_part_1 + self.sep + text_part_2\n",
    "        else:\n",
    "            return text_part_1 + text_part_2\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "          \n",
    "    \n",
    "model_name = initial_model # output_model + \"-mlm\"\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "\n",
    "dataset = CoherenceDataset(texts, preprocess_text, f\" {tokenizer.sep_token} \")\n",
    "\n",
    "epochs = 20\n",
    "batch_size=256\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.0,\n",
    "                              word_masking_proba=0.0,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=0,\n",
    "                              n_encoders=1,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96 * 2,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              classification_head=\"head_ensemble\",\n",
    "                              model=output_model + \"-augsim\",\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "\n",
    "model, optimizer = model_fn()\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=1, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "validate(model, batch_size, dataset, display_detail=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T17:21:48.843337Z",
     "start_time": "2020-08-01T17:15:12.483967Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"finetune\": False,\n",
    "            \"layer\": {\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True,\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        },\n",
    "    },\n",
    "    \"final_layer\": {\n",
    "        \"lr\": optimizer_params[\"lr\"],\n",
    "    }\n",
    "}\n",
    "epochs = 8\n",
    "batch_size=128\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=2e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=2, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "res_cor = validate(model, batch_size, dataset, display_detail=True)\n",
    "res_cor\n",
    "\n",
    "save(model.model, model.tokenizer, output_dir=output_model + \"-cor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSP Dual Model\n",
    "\n",
    "- This task hasn't shown usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T14:55:04.152934Z",
     "start_time": "2020-08-01T14:17:58.542634Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "import random\n",
    "\n",
    "from facebook_hateful_memes_detector.preprocessing import NegativeSamplingDataset\n",
    "\n",
    "from torchtext.data import Dataset, TabularDataset, Field\n",
    "dataset = Dataset(texts, [(\"text\", Field())])\n",
    "ns_dataset = NegativeSamplingDataset(dataset, 2)\n",
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.8, 0.1], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"word_cutout\": 0.8, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.8, 0.1], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.2, \"half_cut\":0.0, \"part_select\": 0.8}\n",
    "    sentence_level = TextAugment([0.8, 0.2], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "class BertDualClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, n_tokens_in, word_masking_proba, preprocess_text):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.preprocess_text = preprocess_text\n",
    "        if type(self.model) == str:\n",
    "            self.model = AutoModel.from_pretrained(self.model)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "            \n",
    "        self.classifier = nn.Sequential(nn.Linear(768 * 2, 256), nn.LeakyReLU(), nn.Linear(256, 2))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.no_grad = True\n",
    "        self.n_tokens_in = n_tokens_in\n",
    "        self.word_masking_proba = word_masking_proba\n",
    "        \n",
    "    def tokenise(self, texts: List[str]):\n",
    "        texts = [self.preprocess_text(t) for t in texts]\n",
    "        tokenizer = self.tokenizer\n",
    "        n_tokens_in = self.n_tokens_in\n",
    "        if self.training and self.word_masking_proba > 0:\n",
    "            texts = [random_word_mask(t, tokenizer, self.word_masking_proba) for t in texts]\n",
    "        converted_texts = tokenizer.batch_encode_plus(texts, add_special_tokens=True, pad_to_max_length=True, max_length=n_tokens_in, truncation=True)\n",
    "        input_ids, attention_mask = converted_texts[\"input_ids\"], converted_texts[\"attention_mask\"]\n",
    "        return torch.tensor(input_ids).to(get_device()), torch.tensor(attention_mask).to(get_device())\n",
    "        \n",
    "    def forward(self, texts):\n",
    "        inp1 = texts[0]\n",
    "        inp2 = texts[1]\n",
    "        labels = texts[2].to(get_device())\n",
    "        \n",
    "        input_ids_1, attention_mask_1 = self.tokenise(inp1)\n",
    "        input_ids_1, attention_mask_1 = input_ids_1.to(get_device()), attention_mask_1.to(get_device())\n",
    "        input_ids_2, attention_mask_2 = self.tokenise(inp2)\n",
    "        input_ids_2, attention_mask_2 = input_ids_2.to(get_device()), attention_mask_2.to(get_device())\n",
    "        \n",
    "        outputs_1 = self.model(input_ids_1, attention_mask=attention_mask_1)\n",
    "        outputs_2 = self.model(input_ids_2, attention_mask=attention_mask_2)\n",
    "        pooled_output_states_1 = outputs_1[1]\n",
    "        pooled_output_states_2 = outputs_2[1]\n",
    "        \n",
    "        pooled_output = torch.cat([pooled_output_states_1, pooled_output_states_2], 1)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        loss = self.loss(logits, labels.long())\n",
    "        logits = logits.type(torch.float)\n",
    "        return logits,0,0, loss\n",
    "\n",
    "model_name = output_model + \"-cor\"\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.99), eps=1e-08, weight_decay=1e-4)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_fn = model_builder(BertDualClassifier,\n",
    "                         dict(model=model_name, n_tokens_in=96, \n",
    "                              word_masking_proba=0.0, preprocess_text=preprocess_text),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "\n",
    "# 0.13393\n",
    "# 0.087709\n",
    "\n",
    "model, optimizer = model_fn()\n",
    "indices = torch.randperm(len(ns_dataset))\n",
    "indices_small_start = int(0.5 * len(ns_dataset))\n",
    "indices_small_end = int(1.0 * len(ns_dataset))\n",
    "print(indices_small_start, indices_small_end)\n",
    "\n",
    "ns_dataset_sm_train = torch.utils.data.Subset(ns_dataset, indices[:indices_small_start])\n",
    "ns_dataset_sm_test = torch.utils.data.Subset(ns_dataset, indices[indices_small_start: indices_small_end])\n",
    "\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, ns_dataset,\n",
    "                                     model_call_back=None, accumulation_steps=2, plot=True,\n",
    "                                     collate_fn=None,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "validate(model, batch_size, ns_dataset, display_detail=True, collate_fn=None,)\n",
    "\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"finetune\": True,\n",
    "            \"lr\": optimizer_params[\"lr\"] / 5,\n",
    "        },\n",
    "    },\n",
    "    \"classifier\": {\n",
    "        \"lr\": optimizer_params[\"lr\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "model.no_grad = False\n",
    "batch_size = 128\n",
    "epochs = 26\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-5)\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, ns_dataset,\n",
    "                                     model_call_back=None, accumulation_steps=2, plot=True,\n",
    "                                     collate_fn=None,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "res_nsp_dual = validate(model, batch_size, ns_dataset, display_detail=True, collate_fn=None)\n",
    "res_nsp_dual\n",
    "\n",
    "save(model.model, model.tokenizer, output_dir=output_model + \"-nsp-dual\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSP Style Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T14:55:37.296019Z",
     "start_time": "2020-08-01T14:55:04.154962Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from mmf.common.sample import Sample, SampleList\n",
    "import random\n",
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.8, 0.1], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"word_cutout\": 0.8, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.8, 0.1], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.2, \"half_cut\":0.0, \"part_select\": 0.8}\n",
    "    sentence_level = TextAugment([0.8, 0.2], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "class DiscriminatoryTextDataset(Dataset):\n",
    "    def __init__(self, texts, process_text, separator_token):\n",
    "        self.texts = list(texts)\n",
    "        self.process_text = process_text\n",
    "        self.separator_token = separator_token\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        process_text = self.process_text\n",
    "        text = self.texts[item]\n",
    "        separator_token = self.separator_token\n",
    "        if random.random() < 0.2:\n",
    "            if random.random() < 0.3:\n",
    "                return Sample({\"text\": text+separator_token+process_text(text), \"label\": 1})\n",
    "            elif random.random() < 0.7:\n",
    "                return Sample({\"text\": process_text(text)+separator_token+text, \"label\": 1})\n",
    "            else:\n",
    "                return Sample({\"text\": process_text(text)+separator_token+process_text(text), \"label\": 1})\n",
    "        else:\n",
    "            rt = random.sample(self.texts, 1)[0]\n",
    "            if random.random() < 0.3:\n",
    "                return Sample({\"text\": text+separator_token+process_text(rt), \"label\": 0})\n",
    "            elif random.random() < 0.7:\n",
    "                return Sample({\"text\": process_text(text)+separator_token+rt, \"label\": 0})\n",
    "            else:\n",
    "                return Sample({\"text\": process_text(text)+separator_token+process_text(rt), \"label\": 0})\n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "model_name = output_model + \"-nsp-dual\" # \"-augsim\"\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "\n",
    "\n",
    "dataset = DiscriminatoryTextDataset(texts, preprocess_text, f\" {tokenizer.sep_token} \")\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.0,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=0,\n",
    "                              n_encoders=1,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96*2,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              classification_head=\"head_ensemble\",\n",
    "                              model=model_name,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "\n",
    "# 0.13393\n",
    "# 0.087709\n",
    "\n",
    "model, optimizer = model_fn()\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=1, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "validate(model, batch_size, dataset, display_detail=True)\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"finetune\": True,\n",
    "            \"lr\": optimizer_params[\"lr\"] / 5,\n",
    "        },\n",
    "    },\n",
    "    \"final_layer\": {\n",
    "        \"lr\": optimizer_params[\"lr\"],\n",
    "    }\n",
    "}\n",
    "epochs = 5\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "train_losses, learning_rates = train(model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=1, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "res_nsp = validate(model, batch_size, dataset, display_detail=True)\n",
    "res_nsp\n",
    "\n",
    "save(model.model, model.tokenizer, output_dir=output_model + \"-nsp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAP - Cluster Assignment Prediction\n",
    "- Assign clusters using one view/Aug\n",
    "- Use other Augs and predict the previous assignment\n",
    "- This is very similar to AugSim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "822px",
    "left": "0px",
    "top": "111.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

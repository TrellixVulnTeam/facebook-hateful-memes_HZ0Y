{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://github.com/facebookresearch/detectron2/blob/master/demo/predictor.py\n",
    "\n",
    "https://github.com/facebookresearch/detectron2/blob/master/detectron2/engine/defaults.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch1.5/index.html\n",
    "!pip install cython \n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "!wget https://raw.githubusercontent.com/facebookresearch/detectron2/master/configs/Base-RCNN-FPN.yaml -O ../Base-RCNN-FPN.yaml\n",
    "!wget https://raw.githubusercontent.com/facebookresearch/detectron2/master/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\n",
    "!wget https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\n",
    "!pip install addict > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:39:23.496486Z",
     "start_time": "2020-06-16T16:39:23.491350Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "DIR = os.getcwd()\n",
    "import torch\n",
    "\n",
    "def print_code(func):\n",
    "    import inspect\n",
    "    from pygments import highlight\n",
    "    from pygments.lexers import PythonLexer\n",
    "    from pygments.formatters import TerminalFormatter\n",
    "\n",
    "    code = \"\".join(inspect.getsourcelines(func)[0])\n",
    "    print(highlight(code, PythonLexer(), TerminalFormatter()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T14:28:37.507399Z",
     "start_time": "2020-06-16T14:28:37.504650Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETECTRON 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:59:13.554897Z",
     "start_time": "2020-06-16T16:59:13.540893Z"
    }
   },
   "outputs": [],
   "source": [
    "import atexit\n",
    "import bisect\n",
    "import multiprocessing as mp\n",
    "from collections import deque\n",
    "import cv2\n",
    "import torch\n",
    "from detectron2.modeling import build_model\n",
    "import detectron2.data.transforms as T\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.utils.video_visualizer import VideoVisualizer\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "\n",
    "class DefaultPredictor:\n",
    "    \"\"\"\n",
    "    Create a simple end-to-end predictor with the given config that runs on\n",
    "    single device for a single input image.\n",
    "    Compared to using the model directly, this class does the following additions:\n",
    "    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n",
    "    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n",
    "    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n",
    "    4. Take one input image and produce a single output, instead of a batch.\n",
    "    If you'd like to do anything more fancy, please refer to its source code\n",
    "    as examples to build and use the model manually.\n",
    "    Attributes:\n",
    "        metadata (Metadata): the metadata of the underlying dataset, obtained from\n",
    "            cfg.DATASETS.TEST.\n",
    "    Examples:\n",
    "    ::\n",
    "        pred = DefaultPredictor(cfg)\n",
    "        inputs = cv2.imread(\"input.jpg\")\n",
    "        outputs = pred(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = build_model(self.cfg)\n",
    "        self.model.eval()\n",
    "        self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "        checkpointer = DetectionCheckpointer(self.model)\n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        self.transform_gen = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  \n",
    "            if self.input_format == \"RGB\":\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.transform_gen.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "#             predictions = self.model([inputs])\n",
    "            predictions = inference([inputs])\n",
    "#             print(predictions[0])\n",
    "            predictions = predictions[0]\n",
    "            return predictions\n",
    "\n",
    "\n",
    "\n",
    "class VisualizationDemo(object):\n",
    "    def __init__(self, cfg, instance_mode=ColorMode.IMAGE, parallel=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            instance_mode (ColorMode):\n",
    "            parallel (bool): whether to run the model in different processes from visualization.\n",
    "                Useful since the visualization logic can be slow.\n",
    "        \"\"\"\n",
    "        self.metadata = MetadataCatalog.get(\n",
    "            cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
    "        )\n",
    "        self.cpu_device = torch.device(\"cpu\")\n",
    "        self.instance_mode = instance_mode\n",
    "        self.predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    def run_on_image(self, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "                This is the format used by OpenCV.\n",
    "        Returns:\n",
    "            predictions (dict): the output of the model.\n",
    "            vis_output (VisImage): the visualized image output.\n",
    "        \"\"\"\n",
    "        vis_output = None\n",
    "        predictions = self.predictor(image)\n",
    "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n",
    "        image = image[:, :, ::-1]\n",
    "        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n",
    "        if \"panoptic_seg\" in predictions:\n",
    "            panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
    "            vis_output = visualizer.draw_panoptic_seg_predictions(\n",
    "                panoptic_seg.to(self.cpu_device), segments_info\n",
    "            )\n",
    "        else:\n",
    "            if \"sem_seg\" in predictions:\n",
    "                vis_output = visualizer.draw_sem_seg(\n",
    "                    predictions[\"sem_seg\"].argmax(dim=0).to(self.cpu_device)\n",
    "                )\n",
    "            if \"instances\" in predictions:\n",
    "                instances = predictions[\"instances\"].to(self.cpu_device)\n",
    "                vis_output = visualizer.draw_instance_predictions(predictions=instances)\n",
    "\n",
    "        return predictions, vis_output\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:06:35.892794Z",
     "start_time": "2020-06-16T17:06:33.759274Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger fvcore (DEBUG)>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/16 17:06:33 detectron2]: \u001b[0mArguments: {'config_file': 'faster_rcnn_R_50_FPN_3x.yaml', 'input': ['../data/img/08291.png'], 'output': 'out.png', 'confidence_threshold': 0.2, 'opts': ['MODEL.WEIGHTS', 'model_final_280758.pkl']}\n",
      "\u001b[32m[06/16 17:06:34 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from model_final_280758.pkl\n",
      "\u001b[32m[06/16 17:06:34 fvcore.common.checkpoint]: \u001b[0mReading a file from 'Detectron2 Model Zoo'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proposals =  torch.Size([1000]) tensor([[ 359.0942,  104.0869,  730.7505,  800.0000],\n",
      "        [  77.8388,    0.0000,  455.4151,  744.6434],\n",
      "        [ 679.4937,  143.2169, 1156.5720,  696.7388],\n",
      "        ...,\n",
      "        [  67.5205,   26.8047,  226.5770,  435.4660],\n",
      "        [ 727.7982,  393.4505,  808.8560,  544.7234],\n",
      "        [ 946.8929,   91.1367, 1158.0629,  199.5205]])\n",
      "[Instances(num_instances=20, image_height=800, image_width=1202, fields=[pred_boxes: Boxes(tensor([[7.2101e+02, 1.2948e+02, 1.1464e+03, 7.9710e+02],\n",
      "        [3.1940e+02, 8.4073e+01, 7.2568e+02, 7.8666e+02],\n",
      "        [1.0960e+03, 2.0056e+00, 1.2011e+03, 2.5725e+02],\n",
      "        [7.4143e+02, 7.0263e+00, 1.1175e+03, 2.8232e+02],\n",
      "        [1.0280e+02, 0.0000e+00, 4.5777e+02, 7.1402e+02],\n",
      "        [6.1678e+02, 8.5077e+01, 6.7205e+02, 1.2839e+02],\n",
      "        [1.1870e+03, 7.6405e+02, 1.2020e+03, 7.8767e+02],\n",
      "        [3.5829e+02, 7.0170e+02, 6.2234e+02, 7.9588e+02],\n",
      "        [6.8281e+02, 2.6243e+02, 8.9620e+02, 3.6334e+02],\n",
      "        [1.4678e+02, 6.0500e+01, 4.1265e+02, 2.6315e+02],\n",
      "        [5.8730e+02, 2.0475e+02, 6.1511e+02, 2.3650e+02],\n",
      "        [9.0403e+01, 9.0497e-01, 8.2447e+02, 3.0363e+02],\n",
      "        [4.8385e+02, 7.4174e+02, 5.3974e+02, 7.8185e+02],\n",
      "        [4.9538e+02, 7.6767e+02, 5.6404e+02, 8.0000e+02],\n",
      "        [7.4703e+01, 7.6174e+00, 2.1066e+02, 2.5469e+02],\n",
      "        [1.4661e+02, 5.7184e+01, 4.1862e+02, 2.6433e+02],\n",
      "        [6.1548e+02, 8.5599e+01, 6.7313e+02, 1.3057e+02],\n",
      "        [5.7522e+02, 7.6477e+02, 7.1237e+02, 8.0000e+02],\n",
      "        [3.5464e+02, 6.9817e+02, 6.2337e+02, 7.9520e+02],\n",
      "        [5.8980e+02, 1.5123e+02, 6.4346e+02, 2.1636e+02]])), scores: tensor([0.9956, 0.9953, 0.9587, 0.9581, 0.9229, 0.8990, 0.8049, 0.7076, 0.5275,\n",
      "        0.4848, 0.4834, 0.3942, 0.3284, 0.3194, 0.2898, 0.2594, 0.2590, 0.2487,\n",
      "        0.2203, 0.2186]), pred_classes: tensor([ 0,  0,  0,  0,  0, 32, 32, 45, 13, 24, 32,  0, 47, 47,  0, 26, 47, 73,\n",
      "        56, 41])])]\n",
      "{}\n",
      "\u001b[32m[06/16 17:06:35 detectron2]: \u001b[0m../data/img/08291.png: detected 20 instances in 1.34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import tqdm\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from addict import Dict\n",
    "\n",
    "\n",
    "\n",
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = args.confidence_threshold\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = args.confidence_threshold\n",
    "    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = args.confidence_threshold\n",
    "    cfg.MODEL.DEVICE = str(device)\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "args = {\"config_file\": \"faster_rcnn_R_50_FPN_3x.yaml\", \n",
    "        \"input\": [\"../data/img/08291.png\"], \"output\": \"out.png\", \"confidence_threshold\": 0.2, \n",
    "       \"opts\": [\"MODEL.WEIGHTS\", \"model_final_280758.pkl\"]}\n",
    "\n",
    "args = Dict(args)\n",
    "\n",
    "\n",
    "\n",
    "# mp.set_start_method(\"spawn\", force=True)\n",
    "# args = get_parser().parse_args()\n",
    "setup_logger(name=\"fvcore\")\n",
    "logger = setup_logger()\n",
    "logger.info(\"Arguments: \" + str(args))\n",
    "\n",
    "cfg = setup_cfg(args)\n",
    "\n",
    "demo = VisualizationDemo(cfg)\n",
    "\n",
    "if args.input:\n",
    "    if len(args.input) == 1:\n",
    "        args.input = glob.glob(os.path.expanduser(args.input[0]))\n",
    "        assert args.input, \"The input path(s) was not found\"\n",
    "    for path in tqdm.tqdm(args.input, disable=not args.output):\n",
    "        # use PIL, to be consistent with evaluation\n",
    "        img = read_image(path, format=\"BGR\")\n",
    "        start_time = time.time()\n",
    "        predictions, visualized_output = demo.run_on_image(img)\n",
    "        logger.info(\n",
    "            \"{}: {} in {:.2f}s\".format(\n",
    "                path,\n",
    "                \"detected {} instances\".format(len(predictions[\"instances\"]))\n",
    "                if \"instances\" in predictions\n",
    "                else \"finished\",\n",
    "                time.time() - start_time,\n",
    "            )\n",
    "        )\n",
    "        visualized_output.save(args.output)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:56:24.582633Z",
     "start_time": "2020-06-16T16:56:22.367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image.open(\"../data/img/08291.png\")\n",
    "# Image.open(\"out.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T17:06:30.283415Z",
     "start_time": "2020-06-16T17:06:30.277607Z"
    }
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "\n",
    "dt2m = demo.predictor.model\n",
    "def inference(batched_inputs, do_postprocess=True):\n",
    "    \"\"\"\n",
    "    Run inference on the given inputs.\n",
    "\n",
    "    Args:\n",
    "        batched_inputs (list[dict]): same as in :meth:`forward`\n",
    "        detected_instances (None or list[Instances]): if not None, it\n",
    "            contains an `Instances` object per image. The `Instances`\n",
    "            object contains \"pred_boxes\" and \"pred_classes\" which are\n",
    "            known boxes in the image.\n",
    "            The inference will then skip the detection of bounding boxes,\n",
    "            and only predict other per-ROI outputs.\n",
    "        do_postprocess (bool): whether to apply post-processing on the outputs.\n",
    "\n",
    "    Returns:\n",
    "        same as in :meth:`forward`.\n",
    "    \"\"\"\n",
    "\n",
    "    images = dt2m.preprocess_image(batched_inputs)\n",
    "    features = dt2m.backbone(images.tensor)\n",
    "    \n",
    "    proposals, _ = dt2m.proposal_generator(images, features, None)\n",
    "    print(\"Proposals = \", proposals[0].objectness_logits.size(), proposals[0].proposal_boxes.tensor)\n",
    "    \n",
    "    results, x = dt2m.roi_heads(images, features, proposals, None)\n",
    "    print(results)\n",
    "    print(x)\n",
    "\n",
    "    if do_postprocess:\n",
    "        return GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n",
    "    else:\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:51:06.967273Z",
     "start_time": "2020-06-16T16:51:06.963019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[90m@staticmethod\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_postprocess\u001b[39;49;00m(instances, batched_inputs, image_sizes):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Rescale the output instances to the target size.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[37m# note: private function; subject to changes\u001b[39;49;00m\n",
      "        processed_results = []\n",
      "        \u001b[34mfor\u001b[39;49;00m results_per_image, input_per_image, image_size \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(\n",
      "            instances, batched_inputs, image_sizes\n",
      "        ):\n",
      "            height = input_per_image.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mheight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, image_size[\u001b[34m0\u001b[39;49;00m])\n",
      "            width = input_per_image.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mwidth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, image_size[\u001b[34m1\u001b[39;49;00m])\n",
      "            r = detector_postprocess(results_per_image, height, width)\n",
      "            processed_results.append({\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: r})\n",
      "        \u001b[34mreturn\u001b[39;49;00m processed_results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_code(GeneralizedRCNN._postprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:39:36.038256Z",
     "start_time": "2020-06-16T16:39:35.978421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32minference\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batched_inputs, detected_instances=\u001b[34mNone\u001b[39;49;00m, do_postprocess=\u001b[34mTrue\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Run inference on the given inputs.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            batched_inputs (list[dict]): same as in :meth:`forward`\u001b[39;49;00m\n",
      "\u001b[33m            detected_instances (None or list[Instances]): if not None, it\u001b[39;49;00m\n",
      "\u001b[33m                contains an `Instances` object per image. The `Instances`\u001b[39;49;00m\n",
      "\u001b[33m                object contains \"pred_boxes\" and \"pred_classes\" which are\u001b[39;49;00m\n",
      "\u001b[33m                known boxes in the image.\u001b[39;49;00m\n",
      "\u001b[33m                The inference will then skip the detection of bounding boxes,\u001b[39;49;00m\n",
      "\u001b[33m                and only predict other per-ROI outputs.\u001b[39;49;00m\n",
      "\u001b[33m            do_postprocess (bool): whether to apply post-processing on the outputs.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\n",
      "\u001b[33m            same as in :meth:`forward`.\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.training\n",
      "\n",
      "        images = \u001b[36mself\u001b[39;49;00m.preprocess_image(batched_inputs)\n",
      "        features = \u001b[36mself\u001b[39;49;00m.backbone(images.tensor)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m detected_instances \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "            \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.proposal_generator:\n",
      "                proposals, _ = \u001b[36mself\u001b[39;49;00m.proposal_generator(images, features, \u001b[34mNone\u001b[39;49;00m)\n",
      "            \u001b[34melse\u001b[39;49;00m:\n",
      "                \u001b[34massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mproposals\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m batched_inputs[\u001b[34m0\u001b[39;49;00m]\n",
      "                proposals = [x[\u001b[33m\"\u001b[39;49;00m\u001b[33mproposals\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m batched_inputs]\n",
      "\n",
      "            results, _ = \u001b[36mself\u001b[39;49;00m.roi_heads(images, features, proposals, \u001b[34mNone\u001b[39;49;00m)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            detected_instances = [x.to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m detected_instances]\n",
      "            results = \u001b[36mself\u001b[39;49;00m.roi_heads.forward_with_given_boxes(features, detected_instances)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m do_postprocess:\n",
      "            \u001b[34mreturn\u001b[39;49;00m GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mreturn\u001b[39;49;00m results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_code(demo.predictor.model.inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T16:39:41.378229Z",
     "start_time": "2020-06-16T16:39:41.366701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, batched_inputs):\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\n",
      "\u001b[33m        Args:\u001b[39;49;00m\n",
      "\u001b[33m            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\u001b[39;49;00m\n",
      "\u001b[33m                Each item in the list contains the inputs for one image.\u001b[39;49;00m\n",
      "\u001b[33m                For now, each item in the list is a dict that contains:\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m                * image: Tensor, image in (C, H, W) format.\u001b[39;49;00m\n",
      "\u001b[33m                * instances (optional): groundtruth :class:`Instances`\u001b[39;49;00m\n",
      "\u001b[33m                * proposals (optional): :class:`Instances`, precomputed proposals.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m                Other information that's included in the original dicts, such as:\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\u001b[39;49;00m\n",
      "\u001b[33m                  See :meth:`postprocess` for details.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m        Returns:\u001b[39;49;00m\n",
      "\u001b[33m            list[dict]:\u001b[39;49;00m\n",
      "\u001b[33m                Each dict is the output for one input image.\u001b[39;49;00m\n",
      "\u001b[33m                The dict contains one key \"instances\" whose value is a :class:`Instances`.\u001b[39;49;00m\n",
      "\u001b[33m                The :class:`Instances` object has the following keys:\u001b[39;49;00m\n",
      "\u001b[33m                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\u001b[39;49;00m\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.training:\n",
      "            \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.inference(batched_inputs)\n",
      "\n",
      "        images = \u001b[36mself\u001b[39;49;00m.preprocess_image(batched_inputs)\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m batched_inputs[\u001b[34m0\u001b[39;49;00m]:\n",
      "            gt_instances = [x[\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m batched_inputs]\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mtargets\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m batched_inputs[\u001b[34m0\u001b[39;49;00m]:\n",
      "            log_first_n(\n",
      "                logging.WARN, \u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mtargets\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m in the model inputs is now renamed to \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, n=\u001b[34m10\u001b[39;49;00m\n",
      "            )\n",
      "            gt_instances = [x[\u001b[33m\"\u001b[39;49;00m\u001b[33mtargets\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m batched_inputs]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            gt_instances = \u001b[34mNone\u001b[39;49;00m\n",
      "\n",
      "        features = \u001b[36mself\u001b[39;49;00m.backbone(images.tensor)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.proposal_generator:\n",
      "            proposals, proposal_losses = \u001b[36mself\u001b[39;49;00m.proposal_generator(images, features, gt_instances)\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mproposals\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[35min\u001b[39;49;00m batched_inputs[\u001b[34m0\u001b[39;49;00m]\n",
      "            proposals = [x[\u001b[33m\"\u001b[39;49;00m\u001b[33mproposals\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m batched_inputs]\n",
      "            proposal_losses = {}\n",
      "\n",
      "        _, detector_losses = \u001b[36mself\u001b[39;49;00m.roi_heads(images, features, proposals, gt_instances)\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.vis_period > \u001b[34m0\u001b[39;49;00m:\n",
      "            storage = get_event_storage()\n",
      "            \u001b[34mif\u001b[39;49;00m storage.iter % \u001b[36mself\u001b[39;49;00m.vis_period == \u001b[34m0\u001b[39;49;00m:\n",
      "                \u001b[36mself\u001b[39;49;00m.visualize_training(batched_inputs, proposals)\n",
      "\n",
      "        losses = {}\n",
      "        losses.update(detector_losses)\n",
      "        losses.update(proposal_losses)\n",
      "        \u001b[34mreturn\u001b[39;49;00m losses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_code(demo.predictor.model.forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

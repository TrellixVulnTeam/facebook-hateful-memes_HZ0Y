{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:39:47.960590Z",
     "start_time": "2020-09-02T07:39:44.663094Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "import os\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "set_global(\"dataloader_workers\", 4)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, my_collate\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What Augs are useful\n",
    "- What Text models perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:39:48.086928Z",
     "start_time": "2020-09-02T07:39:47.964942Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.65, 0.25], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.9], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.3, \"half_cut\":0.0, \"part_select\": 0.75}\n",
    "    sentence_level = TextAugment([0.55, 0.45], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "preprocess_text_test = get_preprocess_text()\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "\n",
    "# Clean text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:39:48.091935Z",
     "start_time": "2020-09-02T07:39:48.088573Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scheduler_init_fn = get_multistep_lr([11, 13], gamma=0.25) # get_cosine_schedule_with_warmup # get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "scheduler_init_fn = get_constant_schedule_with_warmup(0.3)\n",
    "# scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "reg_sched = get_regularizer_scheduler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T17:23:50.676444Z",
     "start_time": "2020-08-14T17:23:50.667000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sgd = torch.optim.SGD\n",
    "sgd_params = dict(lr=2e-2, momentum=0.9, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "rangerQH = optim.RangerQH\n",
    "rangerQHparams = dict(lr=1e-3, betas=(0.9, 0.999), nus=(.7, 1.0),\n",
    "    weight_decay=0.0,\n",
    "    k=6,\n",
    "    alpha=.5,\n",
    "    decouple_weight_decay=True,\n",
    "    eps=1e-8,)\n",
    "\n",
    "adam = torch.optim.Adam\n",
    "adam_params = params=dict(lr=1e-3, weight_decay=1e-7)\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-2)\n",
    "\n",
    "novograd = optim.NovoGrad\n",
    "novograd_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    grad_averaging=False,\n",
    "    amsgrad=False,)\n",
    "\n",
    "qhadam = optim.QHAdam\n",
    "qhadam_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    nus=(1.0, 1.0),\n",
    "    weight_decay=0,\n",
    "    decouple_weight_decay=False,\n",
    "    eps=1e-8,)\n",
    "\n",
    "radam = optim.RAdam\n",
    "radam_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,)\n",
    "\n",
    "yogi = optim.Yogi\n",
    "yogi_params = dict(lr= 1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-3,\n",
    "    initial_accumulator=1e-6,\n",
    "    weight_decay=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Non Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:47:48.405330Z",
     "start_time": "2020-07-29T12:31:41.935915Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"cnn1d\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[1, 4, 7, 9, 11, 14, 17, 19, 23, 27, 31, 34, 37, 41, 44, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.851\t0.666\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T05:40:53.859654Z",
     "start_time": "2020-07-30T05:30:28.121678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"cnn1d\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.05,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[1, 4, 7, 9, 11, 14, 17, 19, 23, 27, 31, 34, 37, 41, 44, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Head Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:38:32.613871Z",
     "start_time": "2020-07-29T11:25:14.832562Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1, # 0.1\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"head_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.05,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 32, 37, 41, 44, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-eval Head Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T19:16:20.346007Z",
     "start_time": "2020-07-28T19:16:20.009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.1,\n",
    "                              word_masking_proba=0.1, # 0.1\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"head_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 32, 37, 41, 44, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=5, evaluate_in_train_mode=True\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.799\t0.657 gaussian_noise=0.0, dropout=0.05, word_masking_proba=0.1,\n",
    "# 0.780\t0.634 gaussian_noise=0.0, dropout=0.05, word_masking_proba=0.15, \n",
    "# 0.806\t0.648 gaussian_noise=0.0, dropout=0.02, word_masking_proba=0.1\n",
    "# 0.803\t0.655 gaussian_noise=0.0, dropout=0.1, word_masking_proba=0.1, (0.807\t0.657) 0.803\t0.653\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train All Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:09:57.651723Z",
     "start_time": "2020-08-01T11:45:13.579574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.01,\n",
    "                              dropout=0.025,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.854\t0.654 0.651 0.646 x2\n",
    "# MLM = 0.854\t0.646\n",
    "# No focal 0.851\t0.647 (0.766\t0.556)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T12:42:49.555891Z",
     "start_time": "2020-08-01T12:13:30.889714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/4,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/2,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/2,\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 28\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.01,\n",
    "                              dropout=0.1,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r3, p3 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.853\t0.655 (0.765\t0.570)\n",
    "# 0.863\t0.651 (0.778\t0.570)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train Head First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T16:42:15.996751Z",
     "start_time": "2020-08-16T16:38:51.397984Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.8, 0.1], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 1.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"word_cutout\": 0.0, \"gibberish_insert\": 0.0}\n",
    "    word_level = TextAugment([0.1, 0.8, 0.1], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\", idf_file=\"/home/ahemf/cache/tfidf_terms.csv\")\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.3, \"half_cut\":0.0, \"part_select\": 0.75}\n",
    "    sentence_level = TextAugment([0.75, 0.25], sentence_level)\n",
    "    gibberish = {\"gibberish_insert\": 0.25, \"punctuation_insert\": 0.75, \n",
    "                 \"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5,}\n",
    "    gibberish = TextAugment([0.75, 0.25], gibberish)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        text = gibberish(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "preprocess_text_test = get_preprocess_text()\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T16:42:16.001462Z",
     "start_time": "2020-08-16T16:42:15.998442Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def aug_sample(sample):\n",
    "    sample = sample.copy()\n",
    "    sample.text = preprocess_text(sample.text)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-16T17:38:47.865903Z",
     "start_time": "2020-08-16T16:42:16.003129Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consistency_loss_weight=0.25\n",
    "num_classes = 2\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.01,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-smclr',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.25,\n",
    "                              attention_drop_proba=0.01,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "\n",
    "model, optimizer = model_fn()\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=None, # reg_sched\n",
    "    validation_epochs=[15, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=None,\n",
    "    prediction_iters=1, evaluate_in_train_mode=False,\n",
    "    consistency_loss_weight=consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "adamw_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer_params = adamw_params\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "\n",
    "epochs = 24\n",
    "batch_size=128\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=None, # reg_sched\n",
    "    validation_epochs=[7, 15, 21, 31, 41, 46, 51, 54, 62],\n",
    "    show_model_stats=False,\n",
    "    accumulation_steps=2,\n",
    "    sampling_policy=None,\n",
    "    prediction_iters=1, evaluate_in_train_mode=False,\n",
    "    consistency_loss_weight=consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "r4, p4 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 48-96\n",
    "# 0.842\t0.658 (0.767\t0.572)\n",
    "# 0.847\t0.652 (0.766\t0.574)\n",
    "\n",
    "# Very high W/C\n",
    "# 0.841\t0.652 (0.760\t0.570)\n",
    "# 0.840\t0.653 (0.759\t0.576)\n",
    "# 0.839\t0.650 (0.760\t0.568)\n",
    "# 0.842\t0.645 (0.762\t0.570)\n",
    "# 0.839\t0.653 (0.761\t0.574)\n",
    "\n",
    "\n",
    "# # No stops + Very High W/C Reg\n",
    "# 0.836\t0.652 (0.754\t0.588)\n",
    "# 0.839\t0.650 (0.762\t0.576)\n",
    "\n",
    "# 48-64 0.0 gauss\n",
    "# 0.848\t0.658 (0.768\t0.578)\n",
    "# 0.849\t0.656 (0.766\t0.570)\n",
    "\n",
    "\n",
    "# SimCLR\n",
    "# 0.856\t0.657 (0.767\t0.590)\n",
    "# 0.859\t0.654 (0.778\t0.568)\n",
    "\n",
    "# 0.828\t0.651 (0.765\t0.566)\n",
    "# Replace multiple puncts with space and strip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-15T09:40:11.585853Z",
     "start_time": "2020-08-15T09:36:29.032500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preds, probas = [], []\n",
    "dataset = convert_dataframe_to_dataset(data[\"dev\"], data[\"metadata\"], True)\n",
    "from tqdm.auto import tqdm, trange\n",
    "for i in trange(100):\n",
    "    proba_list, all_probas_list, predictions_list, labels_list = generate_predictions(model, 128, dataset)\n",
    "    probas.append(all_probas_list)\n",
    "    preds.append(predictions_list)\n",
    "    \n",
    "from collections import Counter\n",
    "preds_voted = [Counter(p).most_common()[0][0] for p in zip(*preds)]\n",
    "probas_mean = torch.tensor(probas).mean(0)\n",
    "pred_probas = probas_mean.max(dim=1).indices\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "print(accuracy_score(labels_list, preds_voted))\n",
    "print(accuracy_score(labels_list, pred_probas))\n",
    "print(roc_auc_score(labels_list, probas_mean[:, 1].tolist(), multi_class=\"ovo\", average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-eval decoder ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:39:51.595972Z",
     "start_time": "2020-09-02T07:39:51.591874Z"
    }
   },
   "outputs": [],
   "source": [
    "# For UDA major keep consistency_loss_weight low since we have less labels and weightage of label prediction should be good\n",
    "# For UDA minor keep consistency loss weight high\n",
    "minor_consistency_loss_weight=0.1\n",
    "major_consistency_loss_weight=0.5\n",
    "num_classes = 2\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:40:05.939416Z",
     "start_time": "2020-09-02T07:39:55.301913Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.01,\n",
    "                              dropout=0.01, # 0.05\n",
    "                              word_masking_proba=0.15,\n",
    "                              whole_word_masking=True,\n",
    "                              uda=True,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-smclr',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.25, # 0.5\n",
    "                              attention_drop_proba=0.0, # 0.1\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=dict(),\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "model, _ = model_fn()\n",
    "\n",
    "## Round Robin Fine tuning of distilbert-smclr and this model as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDA Major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T07:53:12.331508Z",
     "start_time": "2020-09-02T07:40:05.941279Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.4, 0.5], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.1, 0.4, 0.5], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \"glove_twitter\": 0.75,\"word_cutout\": 0.5,\n",
    "                      \"one_third_cut\": 0.25, \"half_cut\":0.0, \"part_select\": 0.25, }\n",
    "    sentence_level = TextAugment([0.1, 0.9], sentence_level, idf_file=\"/home/ahemf/cache/tfidf_terms.csv\"\n",
    "                                )\n",
    "    gibberish = {\"gibberish_insert\": 0.25, \"punctuation_insert\": 0.75, \n",
    "                 \"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5,}\n",
    "    gibberish = TextAugment([0.25, 0.75], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/fdab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        if random.random() < 0.25:\n",
    "            text = sentence_level(text, **kwargs)\n",
    "        else:\n",
    "            text = translation(text, **kwargs)\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "preprocess_text_test = get_preprocess_text()\n",
    "\n",
    "def aug_sample(sample):\n",
    "    sample = sample.copy()\n",
    "    sample.text = preprocess_text(sample.text, identifier=sample.id)\n",
    "    return sample\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "\n",
    "data[\"train\"] = data[\"train\"].sample(frac=1.0)\n",
    "df_train = data[\"train\"].copy(deep=True)\n",
    "df_test = data[\"test\"].copy(deep=True)\n",
    "data[\"test\"] = pd.concat((data[\"train\"].iloc[len(data[\"train\"])//4:].drop(columns=[\"label\"]), data[\"test\"]))\n",
    "data[\"train\"] = data[\"train\"].iloc[:len(data[\"train\"])//4]\n",
    "\n",
    "data[\"train\"].shape\n",
    "data[\"test\"].shape\n",
    "data[\"dev\"].shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T09:45:40.574663Z",
     "start_time": "2020-09-02T07:53:12.334451Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "results, prfs, _ = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    accumulation_steps=1,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 15, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False,\n",
    "    consistency_loss_weight=major_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "adamw_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer_params = adamw_params\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "batch_size=128\n",
    "kfold = False\n",
    "submission, text_model, val_stats_uda_major_1 = train_and_predict(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 7, 15, 21, 26, 30, 35, 41, 46, 51, 54, 60, 65, 70],\n",
    "    show_model_stats=False,\n",
    "    accumulation_steps=4,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    consistency_loss_weight=major_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T11:25:47.663027Z",
     "start_time": "2020-09-02T09:45:40.576828Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"train\"] = df_train\n",
    "data[\"test\"] = df_test\n",
    "data[\"test\"] = pd.concat((data[\"train\"].iloc[:len(data[\"train\"]) - len(data[\"train\"])//4].drop(columns=[\"label\"]), data[\"test\"]))\n",
    "data[\"train\"] = data[\"train\"].iloc[len(data[\"train\"]) - len(data[\"train\"])//4:]\n",
    "\n",
    "data[\"train\"].shape\n",
    "data[\"test\"].shape\n",
    "data[\"dev\"].shape\n",
    "\n",
    "epochs = 30\n",
    "batch_size=128\n",
    "kfold = False\n",
    "submission, text_model, val_stats_uda_major_2 = train_and_predict(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 7, 15, 21, 26, 30, 35, 41, 46, 51, 54, 60, 65, 70],\n",
    "    show_model_stats=False,\n",
    "    accumulation_steps=4,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    consistency_loss_weight=major_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDA Minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T11:25:47.766172Z",
     "start_time": "2020-09-02T11:25:47.665759Z"
    }
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# def get_preprocess_text():\n",
    "#     char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "#                   \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "#     char_level = TextAugment([0.1, 0.6, 0.3], char_level)\n",
    "#     word_level = {\"split\": 0.2,\n",
    "#                  \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "#     word_level = TextAugment([0.1, 0.6, 0.3], word_level, \n",
    "#                              fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "#     sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \"glove_twitter\": 0.75,\"word_cutout\": 0.5,\n",
    "#                       \"one_third_cut\": 0.25, \"half_cut\":0.0, \"part_select\": 0.25, }\n",
    "#     sentence_level = TextAugment([0.25, 0.75], sentence_level, idf_file=\"/home/ahemf/cache/tfidf_terms.csv\"\n",
    "#                                 )\n",
    "#     gibberish = {\"gibberish_insert\": 0.25, \"punctuation_insert\": 0.75, \n",
    "#                  \"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5,}\n",
    "#     gibberish = TextAugment([0.5, 0.5], gibberish)\n",
    "#     translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "#     translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/fdab.csv\")\n",
    "#     def process(text, **kwargs):\n",
    "#         if random.random() < 0.5:\n",
    "#             text = sentence_level(text, **kwargs)\n",
    "#         else:\n",
    "#             text = translation(text, **kwargs)\n",
    "#         text = word_level(text, **kwargs)\n",
    "#         text = char_level(text, **kwargs)\n",
    "#         text = gibberish(text, **kwargs)\n",
    "#         return text\n",
    "#     return process\n",
    "\n",
    "\n",
    "# preprocess_text = get_preprocess_text()\n",
    "# preprocess_text_test = get_preprocess_text()\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=False,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "def aug_sample(sample):\n",
    "    sample = sample.copy()\n",
    "    sample.text = preprocess_text(sample.text, identifier=sample.id)\n",
    "    return sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "batch_size = 128\n",
    "results, prfs, _ = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    accumulation_steps=1,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 15, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False,\n",
    "    consistency_loss_weight=minor_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "adamw_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer_params = adamw_params\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T13:51:05.795724Z",
     "start_time": "2020-09-02T11:25:47.767906Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 40\n",
    "batch_size=128\n",
    "kfold = False\n",
    "submission, text_model, val_stats = train_and_predict(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 7, 15, 21, 26, 30, 35, 40, 46, 51, 54, 60, 65, 70],\n",
    "    show_model_stats=False,\n",
    "    accumulation_steps=4,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    consistency_loss_weight=minor_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "submission.to_csv(\"submission.csv\",index=False)\n",
    "submission.sample(5)\n",
    "\n",
    "# 0.722\t0.729\n",
    "# Epoch =  7 Train = ['65.58', '70.96', '76.21'] Val = ['63.17', '59.00', '65.39']\n",
    "# Epoch =  15 Train = ['71.83', '74.37', '80.36'] Val = ['65.55', '56.40', '69.26']\n",
    "# Epoch =  21 Train = ['74.72', '76.03', '82.47'] Val = ['67.23', '58.80', '70.98']\n",
    "# Epoch =  24 Train = ['76.21', '76.28', '83.56'] Val = ['68.91', '61.60', '72.54']\n",
    "# Epoch =  27 Train = ['77.27', '76.88', '84.36'] Val = ['70.52', '62.20', '73.87']\n",
    "# Epoch =  30 Train = ['78.28', '77.12', '85.13'] Val = ['71.45', '62.40', '74.26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-02T15:15:57.580501Z",
     "start_time": "2020-09-02T14:55:22.514339Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size=128\n",
    "kfold = False\n",
    "submission, text_model, val_stats = train_and_predict(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=None,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[2, 5, 7, 10, 15, 20, 26, 30, 35, 40, 46, 51, 54, 60, 65, 70],\n",
    "    show_model_stats=False,\n",
    "    accumulation_steps=8,\n",
    "    sampling_policy=\"uda_without_replacement\",\n",
    "    consistency_loss_weight=minor_consistency_loss_weight, num_classes=num_classes,\n",
    "    aug_1=aug_sample, aug_2=aug_sample,\n",
    ")\n",
    "submission.to_csv(\"submission_2.csv\",index=False)\n",
    "submission.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "822px",
    "left": "0px",
    "top": "111.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

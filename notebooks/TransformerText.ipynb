{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T08:37:36.178022Z",
     "start_time": "2020-07-31T08:37:33.260466Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "set_global(\"dataloader_workers\", 4)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, my_collate\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T08:37:36.189001Z",
     "start_time": "2020-07-31T08:37:36.179783Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What Augs are useful\n",
    "- What Text models perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T13:44:01.066056Z",
     "start_time": "2020-07-31T13:44:00.969314Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.05, 0.45, 0.5], char_level)\n",
    "    word_level = {\"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.5, \"word_join\": 0.2, \"word_cutout\": 0.8, \"gibberish_insert\": 0.4}\n",
    "    word_level = TextAugment([0.05, 0.45, 0.5], word_level)\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "                      \"one_third_cut\": 0.3, \"half_cut\":0.2, \"part_select\": 0.75}\n",
    "    sentence_level = TextAugment([0.15, 0.85], sentence_level)\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "\n",
    "# choice_probas = {\"keyboard\": 0.1, \"char_substitute\": 0.0, \"char_insert\": 0.1, \"char_swap\": 0.1, \"ocr\": 0.0, \"char_delete\": 0.1,\n",
    "#                  \"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.1,\n",
    "#                  \"stopword_insert\": 0.3, \"word_join\": 0.2, \"word_cutout\": 0.8,\n",
    "#                  \"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "#                  \"one_third_cut\": 0.3, \"half_cut\":0.0, \"part_select\": 0.5}\n",
    "# preprocess_text = TextAugment([0.05, 0.2, 0.2, 0.3, 0.25], choice_probas, fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "\n",
    "# choice_probas_test = {\"keyboard\": 0.1, \"char_substitute\": 0.0, \"char_insert\": 0.1, \"char_swap\": 0.0, \"char_delete\": 0.2,\n",
    "#                  \"fasttext\": 0.0, \"glove_twitter\": 0.0, \"glove_wiki\": 0.0, \"word2vec\": 0.0, \"split\": 0.1,\n",
    "#                  \"stopword_insert\": 0.3, \"word_join\": 0.2, \"word_cutout\": 0.8,\n",
    "#                  \"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, \n",
    "#                       \"one_third_cut\": 0.3, \"half_cut\":0.0, \"part_select\": 0.5}\n",
    "\n",
    "# preprocess_text_test = TextAugment([0.1, 0.9], choice_probas_test, fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "preprocess_text_test = get_preprocess_text()\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "\n",
    "# Clean text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T13:44:02.640560Z",
     "start_time": "2020-07-31T13:44:02.638056Z"
    }
   },
   "outputs": [],
   "source": [
    "# data[\"train\"].label.value_counts()\n",
    "# train = data[\"train\"]\n",
    "\n",
    "# ones = train[train[\"label\"] == 1]\n",
    "# zeros = train[train[\"label\"] == 0].sample(n=len(ones), replace=False)\n",
    "# data[\"train\"] = pd.concat((ones, zeros)).sample(frac=1.0)\n",
    "# data[\"train\"].label.value_counts()\n",
    "\n",
    "# len(set(data[\"train\"][\"id\"])) == data[\"train\"].shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/how-to-implement-torch-optim-lr-scheduler-cosineannealinglr/28797/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T13:44:03.856450Z",
     "start_time": "2020-07-31T13:44:03.853351Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scheduler_init_fn = get_multistep_lr([11, 13], gamma=0.25) # get_cosine_schedule_with_warmup # get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "reg_sched = get_regularizer_scheduler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T19:00:48.022523Z",
     "start_time": "2020-07-29T19:00:48.013043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sgd = torch.optim.SGD\n",
    "sgd_params = dict(lr=2e-2, momentum=0.9, dampening=0, weight_decay=0, nesterov=False)\n",
    "\n",
    "rangerQH = optim.RangerQH\n",
    "rangerQHparams = dict(lr=1e-3, betas=(0.9, 0.999), nus=(.7, 1.0),\n",
    "    weight_decay=0.0,\n",
    "    k=6,\n",
    "    alpha=.5,\n",
    "    decouple_weight_decay=True,\n",
    "    eps=1e-8,)\n",
    "\n",
    "adam = torch.optim.Adam\n",
    "adam_params = params=dict(lr=1e-3, weight_decay=1e-7)\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-2)\n",
    "\n",
    "novograd = optim.NovoGrad\n",
    "novograd_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    grad_averaging=False,\n",
    "    amsgrad=False,)\n",
    "\n",
    "qhadam = optim.QHAdam\n",
    "qhadam_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    nus=(1.0, 1.0),\n",
    "    weight_decay=0,\n",
    "    decouple_weight_decay=False,\n",
    "    eps=1e-8,)\n",
    "\n",
    "radam = optim.RAdam\n",
    "radam_params = dict(lr= 1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,)\n",
    "\n",
    "yogi = optim.Yogi\n",
    "yogi_params = dict(lr= 1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-3,\n",
    "    initial_accumulator=1e-6,\n",
    "    weight_decay=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Non Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T12:47:48.405330Z",
     "start_time": "2020-07-29T12:31:41.935915Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"cnn1d\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[1, 4, 7, 9, 11, 14, 17, 19, 23, 27, 31, 34, 37, 41, 44, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.851\t0.666\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T05:40:53.859654Z",
     "start_time": "2020-07-30T05:30:28.121678Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"cnn1d\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.05,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[1, 4, 7, 9, 11, 14, 17, 19, 23, 27, 31, 34, 37, 41, 44, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Head Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T11:38:32.613871Z",
     "start_time": "2020-07-29T11:25:14.832562Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1, # 0.1\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"head_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.05,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 32, 37, 41, 44, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-eval Head Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-28T19:16:20.346007Z",
     "start_time": "2020-07-28T19:16:20.009Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.1,\n",
    "                              word_masking_proba=0.1, # 0.1\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=1,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=16,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"head_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 32, 37, 41, 44, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=5, evaluate_in_train_mode=True\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.799\t0.657 gaussian_noise=0.0, dropout=0.05, word_masking_proba=0.1,\n",
    "# 0.780\t0.634 gaussian_noise=0.0, dropout=0.05, word_masking_proba=0.15, \n",
    "# 0.806\t0.648 gaussian_noise=0.0, dropout=0.02, word_masking_proba=0.1\n",
    "# 0.803\t0.655 gaussian_noise=0.0, dropout=0.1, word_masking_proba=0.1, (0.807\t0.657) 0.803\t0.653\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T14:50:14.024767Z",
     "start_time": "2020-07-31T14:28:45.956962Z"
    }
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.01,\n",
    "                              dropout=0.1,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.01,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "# 0.861\t0.647 (0.773\t0.572) gaussian_noise=0.01, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.0,\n",
    "# 0.859\t0.656 (0.768\t0.590) || 0.858\t0.649 (0.770\t0.578) gaussian_noise=0.05, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.0,\n",
    "# 0.858\t0.655 (0.772\t0.566) gaussian_noise=0.05, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.01,\n",
    "# 0.858\t0.649 (0.765\t0.586)  gaussian_noise=0.05, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.02\n",
    "# 0.858\t0.653 (0.770\t0.568) gaussian_noise=0.02, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.01,\n",
    "# 0.855\t0.650 (0.770\t0.566) gaussian_noise=0.05, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.05,\n",
    "# 0.862\t0.646 (0.777\t0.552) gaussian_noise=0.01, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.05,\n",
    "# 0.860\t0.648 (0.774\t0.560) gaussian_noise=0.075, dropout=0.15, word_masking_proba=0.15, attention_drop_proba=0.05,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T13:36:30.116162Z",
     "start_time": "2020-07-31T13:17:57.606113Z"
    }
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 12\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.15,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.05,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "\n",
    "model, optimizer = model_fn()\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[15, 27, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "adamw_params = dict(lr=2e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "\n",
    "epochs = 16\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[15, 21, 27, 31, 34, 41, 47, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T13:17:57.604507Z",
     "start_time": "2020-07-31T12:59:40.355094Z"
    }
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/4,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/2,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/2,\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"final_layer\": {\n",
    "        \"lr\": optimizer_params[\"lr\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.0,\n",
    "                              dropout=0.05,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.0,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[17, 27, 34, 41, 45, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=1, evaluate_in_train_mode=False\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Multi-eval decoder ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T12:38:17.523006Z",
     "start_time": "2020-07-30T12:18:45.516386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=None, \n",
    "                    test_text_transform=preprocess_text_test, test_image_transform=None, \n",
    "                    cache_images = True, use_images = False, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/10,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/10,\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"]/10,\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    },\n",
    "    \"final_layer\": {\n",
    "        \"lr\": optimizer_params[\"lr\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.02,\n",
    "                              dropout=0.1,\n",
    "                              word_masking_proba=0.1,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=64,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.01,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched, # reg_sched\n",
    "    validation_epochs=[19, 27, 37, 41, 46, 51, 54],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    prediction_iters=10, evaluate_in_train_mode=True\n",
    ")\n",
    "r2, p2 = results, prfs\n",
    "results\n",
    "prfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T18:58:41.688615Z",
     "start_time": "2020-07-30T18:58:41.679392Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"lr\": optimizer_params[\"lr\"] / 1000,\n",
    "        \"finetune\": False,\n",
    "        \"encoder\": {\n",
    "            \"layer\": {\n",
    "                \"2\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"3\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"4\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "                \"5\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "epochs = 24\n",
    "batch_size = 256\n",
    "model_fn = model_builder(AlbertClassifer,\n",
    "                         dict(classifier_dims=768,\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.05,\n",
    "                              dropout=0.15,\n",
    "                              word_masking_proba=0.15,\n",
    "                              internal_dims=768,\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              n_layers=2,\n",
    "                              n_encoders=0,\n",
    "                              n_decoders=0,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=48,\n",
    "                              featurizer=\"transformer\",\n",
    "                              model='distilbert-nsp',\n",
    "                              loss=\"focal\",\n",
    "                              classification_head=\"decoder_ensemble\", # head_ensemble\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.5, # 0.1\n",
    "                              attention_drop_proba=0.01,\n",
    "                              finetune=False,\n",
    "                              n_classifier_layers=1,\n",
    "                              n_classifier_decoders=16, # 4\n",
    "                             ),\n",
    "                         per_param_opts_fn=lr_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T19:15:15.272225Z",
     "start_time": "2020-07-30T18:58:43.326823Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission, text_model = train_and_predict(model_fn, data, batch_size, epochs, \n",
    "                                           scheduler_init_fn=scheduler_init_fn, \n",
    "                                           model_call_back=reg_sched,\n",
    "                                           sampling_policy=\"without_replacement\",\n",
    "                                           validation_epochs=[15, 34, 42, 46],\n",
    "                                          )\n",
    "submission.to_csv(\"submission.csv\",index=False)\n",
    "submission.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "822px",
    "left": "0px",
    "top": "111.133px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:55:23.544950Z",
     "start_time": "2020-10-20T11:55:20.625448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "import random\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "# set_global(\"cache_dir\", \"/Users/ahemf/mygit/facebook-hateful-memes/cache\")\n",
    "set_global(\"dataloader_workers\", 32)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, random_word_mask, dict2sampleList, run_simclr, load_stored_params\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment, DefinedAffine, HalfSwap, get_transforms_for_bbox_methods\n",
    "from facebook_hateful_memes_detector.preprocessing import get_transforms_for_multiview\n",
    "from facebook_hateful_memes_detector.preprocessing import NegativeSamplingDataset, ImageFolderDataset, ZipDatasets\n",
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBert import VilBertVisualBertModel\n",
    "from facebook_hateful_memes_detector.models.MultiModal import VilBertVisualBertModelV2, MLMSimCLR, MLMOnlyV2, make_plots\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "from facebook_hateful_memes_detector.utils import get_vgg_face_model, get_torchvision_classification_models, init_fc, my_collate, merge_sample_lists\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "devices = [\"cuda:0\", \"cuda:0\", \"cuda:1\", \"cuda:2\", \"cuda:3\",]\n",
    "# devices = [\"cpu\"] * 5\n",
    "device = torch.device(devices[0] if torch.cuda.is_available() else \"cpu\")\n",
    "device1 = torch.device(devices[1] if torch.cuda.is_available() else \"cpu\")\n",
    "device2 = torch.device(devices[2] if torch.cuda.is_available() else \"cpu\")\n",
    "device3 = torch.device(devices[3] if torch.cuda.is_available() else \"cpu\")\n",
    "device4 = torch.device(devices[4] if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_device(device)\n",
    "print(get_device())\n",
    "\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "scheduler_init_fn = get_constant_schedule_with_warmup()\n",
    "set_global(\"gradient_clipping\", 10)\n",
    "# Use mixup in SSL training, Use UDA maybe\n",
    "# os.path.join(get_global(\"models_dir\"),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:55:24.014388Z",
     "start_time": "2020-10-20T11:55:23.546662Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text():\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.25, 0.75], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        text = translation(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "\n",
    "def get_views():\n",
    "    image_views = get_transforms_for_multiview()\n",
    "    def get_view(imv):\n",
    "        imtrans = image_views[imv]\n",
    "        ag_idx = imv + 1\n",
    "        def aug_sample(sample):\n",
    "            sample[\"text_view_%s\" % ag_idx] = preprocess_text(sample.original_text, identifier=sample.id)\n",
    "            sample[\"image_view_%s\" % ag_idx] = imtrans(sample.original_image)\n",
    "        return aug_sample\n",
    "    \n",
    "    return [get_view(0), get_view(1), get_view(2)]\n",
    "\n",
    "data = get_datasets(data_dir=\"/home/ahemf/cache/data/\",\n",
    "                    train_text_transform=None,\n",
    "                    train_image_transform=None,\n",
    "                    test_text_transform=None,\n",
    "                    test_image_transform=None,\n",
    "                    train_torchvision_pre_image_transform=None,\n",
    "                    test_torchvision_pre_image_transform=None,\n",
    "                    cache_images=False,\n",
    "                    use_images=True,\n",
    "                    dev=False,\n",
    "                    test_dev=True,\n",
    "                    keep_original_text=True,\n",
    "                    keep_original_image=True,\n",
    "                    keep_processed_image=True,\n",
    "                    keep_torchvision_image=False,\n",
    "                    train_mixup_config=None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:55:24.485865Z",
     "start_time": "2020-10-20T11:55:24.016167Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.4, 0.5], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.1, 0.4, 0.5], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, #\"word_cutout\": 0.5, \"glove_twitter\": 0.75, \n",
    "                      \"one_third_cut\": 0.2, \"half_cut\":0.0, \"part_select\": 0.2, }\n",
    "    sentence_level = TextAugment([0.1, 0.9], sentence_level, idf_file=\"/home/ahemf/cache/data/tfidf_terms.csv\"\n",
    "                                )\n",
    "    gibberish = {\"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"punctuation_insert\": 0.25, \"punctuation_continue\": 0.5}\n",
    "    gibberish = TextAugment([0.25, 0.75], gibberish)\n",
    "    translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/data/hard_dab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        if random.random() < 0.2:\n",
    "            text = sentence_level(text, **kwargs)\n",
    "        else:\n",
    "            text = translation(text, **kwargs)\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "def get_views():\n",
    "    image_views = get_transforms_for_multiview()\n",
    "    def get_view(imv):\n",
    "        \n",
    "        imtrans = image_views[imv]\n",
    "        ag_idx = imv + 1\n",
    "        def aug_sample(sample):\n",
    "            sample[\"text_view_%s\" % ag_idx] = preprocess_text(sample.original_text, identifier=sample.id)\n",
    "            sample[\"image_view_%s\" % ag_idx] = imtrans(sample.original_image)\n",
    "        return aug_sample\n",
    "    \n",
    "    return [get_view(0), get_view(1), get_view(2)]\n",
    "\n",
    "# transforms.RandomAffine(0, scale=(0.75, 0.75))\n",
    "# transforms.RandomAffine(0, scale=(1.25, 1.25))\n",
    "# DefinedRotation(15)\n",
    "\n",
    "data = get_datasets(data_dir=\"/home/ahemf/cache/data/\",\n",
    "                    train_text_transform=preprocess_text,\n",
    "                    train_image_transform=get_transforms_for_bbox_methods(),\n",
    "                    test_text_transform=None,\n",
    "                    test_image_transform=None,\n",
    "                    train_torchvision_pre_image_transform=None,\n",
    "                    test_torchvision_pre_image_transform=None,\n",
    "                    cache_images=False,\n",
    "                    use_images=True,\n",
    "                    dev=False,\n",
    "                    test_dev=True,\n",
    "                    keep_original_text=True,\n",
    "                    keep_original_image=True,\n",
    "                    keep_processed_image=True,\n",
    "                    keep_torchvision_image=False,\n",
    "                    train_mixup_config=None)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:55:25.087624Z",
     "start_time": "2020-10-20T11:55:25.072499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12540, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"test\"][\"label\"] = -1\n",
    "data['test_unseen'][\"label\"] = -1\n",
    "dev_unseen = data['dev_unseen'].copy()\n",
    "data['dev_unseen'][\"label\"] = -1\n",
    "\n",
    "df = pd.concat((data[\"train\"],\n",
    "                data['dev_unseen'],\n",
    "                data[\"test\"], data['test_unseen']))\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:55:25.455136Z",
     "start_time": "2020-10-20T11:55:25.448674Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = convert_dataframe_to_dataset(df, data[\"metadata\"], True, additional_processors=get_views())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:56:16.782889Z",
     "start_time": "2020-10-20T11:55:25.891597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n",
      "Overriding option config to projects/hateful_memes/configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n",
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens Out =  132 Classifier Dims =  768 Matches embedding_dims:  True\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(\n",
    "    model_name={\"lxmert\": dict(dropout=0.1, gaussian_noise=0.02, feature_dropout=0.01), \n",
    "                \"vilbert\": dict(dropout=0.05, gaussian_noise=0.01, feature_dropout=0.01), \n",
    "                \"visual_bert\": dict(dropout=0.05, gaussian_noise=0.01, feature_dropout=0.01),\n",
    "                \"mmbt_region\": dict(dropout=0.1, gaussian_noise=0.02, feature_dropout=0.01)},\n",
    "    num_classes=2,\n",
    "    gaussian_noise=0.01,\n",
    "    dropout=0.1,\n",
    "    word_masking_proba=0.15,\n",
    "    featurizer=\"pass\",\n",
    "    final_layer_builder=fb_1d_loss_builder,\n",
    "    internal_dims=768,\n",
    "    classifier_dims=768,\n",
    "    n_tokens_in=96,\n",
    "    n_tokens_out=96,\n",
    "    n_layers=0,\n",
    "    attention_drop_proba=0.0,\n",
    "    loss=\"focal\",\n",
    "    dice_loss_coef=0.0,\n",
    "    auc_loss_coef=0.0,\n",
    "    bbox_swaps=1,\n",
    "    bbox_copies=1,\n",
    "    bbox_deletes=1,\n",
    "    bbox_gaussian_noise=0.01,\n",
    "    bbox_feature_dropout=0.01,\n",
    "    bbox_dropout=0.05,\n",
    "    view_transforms=get_views(),\n",
    "    view_loss_weight=0.1,\n",
    "    devices=dict(lxmert=device1, vilbert=device4, \n",
    "                 visual_bert=device3, mmbt_region=device2),\n",
    "    finetune=False)\n",
    "\n",
    "model_class = VilBertVisualBertModelV2\n",
    "model = model_class(**model_params)\n",
    "# model = model.to(get_device())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:56:16.787401Z",
     "start_time": "2020-10-20T11:56:16.784526Z"
    }
   },
   "outputs": [],
   "source": [
    "del model.vilbert.model.classifier\n",
    "del model.visual_bert.model.classifier\n",
    "del model.mmbt_region.model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:56:18.310788Z",
     "start_time": "2020-10-20T11:56:16.789152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model.lxmert.state_dict(), os.path.join(get_global(\"models_dir\"),\"lxmert-mlm-init.pth\"))\n",
    "model.lxmert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"lxmert-mlm-init.pth\")))\n",
    "\n",
    "# torch.save(model.mmbt_region.state_dict(), os.path.join(get_global(\"models_dir\"),\"mmbt_region-mlm-init.pth\"))\n",
    "model.mmbt_region.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"mmbt_region-mlm-init.pth\")))\n",
    "\n",
    "\n",
    "# torch.save(model.visual_bert.state_dict(), os.path.join(get_global(\"models_dir\"),\"visual_bert-mlm-init.pth\"))\n",
    "model.visual_bert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"visual_bert-mlm-init.pth\")))\n",
    "\n",
    "# torch.save(model.vilbert.state_dict(), os.path.join(get_global(\"models_dir\"),\"vilbert-mlm-init.pth\"))\n",
    "model.vilbert.load_state_dict(torch.load(os.path.join(get_global(\"models_dir\"),\"vilbert-mlm-init.pth\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unimodal MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:56:18.954679Z",
     "start_time": "2020-10-20T11:56:18.312968Z"
    }
   },
   "outputs": [],
   "source": [
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBertV2 import positive, negative\n",
    "mlm_model = MLMOnlyV2(model, 0.1, {1: negative, 0: positive}, None, mlm_loss_weight=0.5,)\n",
    "# mlm_model = mlm_model.to(get_device())\n",
    "# mlm_model.load_state_dict(torch.load(\"mlm-model-v2.pth\"))\n",
    "# mlm_model.load_state_dict(torch.load(\"mlm-model-v2-view-1.pth\"))\n",
    "# mlm_model = mlm_model.to(get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:56:18.958954Z",
     "start_time": "2020-10-20T11:56:18.956407Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(mlm_model.state_dict(), \"mlm-model-v2-view-1.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-20T11:55:29.701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocast =  True Epochs =  2 Divisor = 1 Examples = 12540 Batch Size =  8\n",
      "Training Samples =  12540 Weighted Sampling =  False Num Batches =  1568 Accumulation steps =  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82ecc8d0b2d4b8e92e8f1f7d5eacee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085af1fb8e884cb187b98686d709b1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=1568.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception for:  keyboard | Original Text are the Final Text are the | ['char_insert' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text can ' ' ' ' t be homici de if there is Final Text can ' ' ' ' t be hom$ci de if the!e is | ['char_substitute' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text i do if she got Final Text i do if she got | ['keyboard'] argument of type 'NoneType' is not iterable\n",
      "FeatureExtractor : Loaded Model...\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n",
      "LXMERTFeatureExtractor : Loaded Model...\n",
      "Exception for:  keyboard | Original Text turn to the Final Text tur! to the | ['char_substitute' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text no she'??s Final Text no she'??s | ['char_substitute' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text You'''re an f*g,, you''''re an f*g,,,, you,'re all f*g! Final Text You'''re an f*g,, you''''re an f*g,,,, you,'re all f*g! | ['keyboard' 'char_delete'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text You're F * g, you're af * g, you're f * gs! Final Text You're F * g, you're af * g, you're f * gs! | ['keyboard' 'char_swap'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text The gas was so high. Final Text The gas was so hih. | ['char_delete' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text Fuck it. Final Text Fuc#k it. | ['char_insert' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  dab | Original Text women deserve equal rights and support Final Text women deserve equal rights and support | ['dab'] Sample larger than population or is negative\n",
      "Exception for:  dab | Original Text women deserve equal rights and support Final Text women deserve equal rights and support | ['dab'] Sample larger than population or is negative\n",
      "Exception for:  dab | Original Text women deserve equal rights and support Final Text women deserve equal rights and support | ['dab'] Sample larger than population or is negative\n",
      "Exception for:  dab | Original Text women deserve equal rights and support Final Text women deserve equal rights and support | ['dab'] Sample larger than population or is negative\n",
      "Exception for:  keyboard | Original Text a man Final Text a man | ['keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text Top losers Final Text Top los!rs | ['char_substitute' 'keyboard'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text try not to be Final Text try not to be | ['keyboard' 'char_insert'] argument of type 'NoneType' is not iterable\n",
      "Exception for:  keyboard | Original Text Big Big Mac has ice Final Text Big Big Mac has ice | ['char_swap' 'keyboard'] argument of type 'NoneType' is not iterable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_strategy = {\n",
    "    \"finetune\": True,\n",
    "    \"model\": {\n",
    "        \"vilbert\": {\"finetune\": True,},\n",
    "        \"visual_bert\": {\"finetune\": True,},\n",
    "        \"mmbt_region\": {\"finetune\": True,},\n",
    "        \"lxmert\": {\"finetune\": True,},\n",
    "    },\n",
    "    \"mlm\": {\"finetune\": True},\n",
    "}\n",
    "epochs = 2\n",
    "batch_size = 8\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=2e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "# optimizer_params = dict(lr=2e-5, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "\n",
    "_ = group_wise_finetune(mlm_model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(mlm_model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "train_losses, learning_rates, _ = train(mlm_model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=8, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:15:51.076783Z",
     "start_time": "2020-10-20T11:15:50.162944Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T11:19:49.625588Z",
     "start_time": "2020-10-20T11:19:49.617444Z"
    }
   },
   "outputs": [],
   "source": [
    "cache_stats = get_global(\"cache_stats\")\n",
    "cache_stats['get_img_details']\n",
    "cache_stats['get_lxmert_details']\n",
    "\n",
    "train_stats = get_global(\"train_stats\")\n",
    "train_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-11T18:29:40.513398Z",
     "start_time": "2020-10-11T18:29:40.495558Z"
    }
   },
   "outputs": [],
   "source": [
    "model_parameters = list(filter(lambda p: p.requires_grad, model.lxmert.parameters()))\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"Trainable Params = %s\" % (params), \"\\n\", model.lxmert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T07:35:43.630651Z",
     "start_time": "2020-10-20T07:35:26.688798Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "make_plots(model, mlm_model, logy=False, exclude_from_start=5, smoothing=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T07:51:28.432723Z",
     "start_time": "2020-10-20T07:43:45.236941Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(mlm_model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:00:57.173387Z",
     "start_time": "2020-10-20T07:53:26.296088Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:09:11.889091Z",
     "start_time": "2020-10-20T08:00:57.175061Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev_unseen'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(mlm_model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T08:16:57.260054Z",
     "start_time": "2020-10-20T08:09:11.890868Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_memory()\n",
    "batch_size = 16\n",
    "dev_dataset = convert_dataframe_to_dataset(data['dev_unseen'], data[\"metadata\"], False, additional_processors=get_views())\n",
    "validate(model, batch_size, dev_dataset, display_detail=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T12:20:30.996386Z",
     "start_time": "2020-10-16T11:55:16.755997Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = predict(mlm_model, data, batch_size, competition_phase=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, data, batch_size, competition_phase=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T12:24:45.935653Z",
     "start_time": "2020-10-16T12:24:45.927441Z"
    }
   },
   "outputs": [],
   "source": [
    "preds.to_csv(\"submission_phase_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-eval\n",
    "- Use back-translation\n",
    "    - Longest 3 backtranslation\n",
    "- Use imgaug\n",
    "    - identity\n",
    "    - HFlip\n",
    "    - GrayScale\n",
    "    - GridDrop\n",
    "    \n",
    "- Use word replace\n",
    "    - Replace top 2 longest word\n",
    "- Use `[masking]`\n",
    "    - 0.15 mask rate `5` times\n",
    "- identity text\n",
    "    \n",
    "(5+3+3)*4 = 44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:47:17.732990Z",
     "start_time": "2020-10-10T05:47:14.433291Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "import random\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer, AutoModel\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "# set_global(\"cache_dir\", \"/Users/ahemf/mygit/facebook-hateful-memes/cache\")\n",
    "set_global(\"dataloader_workers\", 8)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, random_word_mask, dict2sampleList, run_simclr, load_stored_params\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, LangFeaturesModel, AlbertClassifer\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment, DefinedAffine, HalfSwap, get_transforms_for_bbox_methods\n",
    "from facebook_hateful_memes_detector.preprocessing import NegativeSamplingDataset, ImageFolderDataset, ZipDatasets\n",
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBert import VilBertVisualBertModel\n",
    "from facebook_hateful_memes_detector.models.MultiModal import VilBertVisualBertModelV2, MLMSimCLR\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "from facebook_hateful_memes_detector.utils import get_vgg_face_model, get_torchvision_classification_models, init_fc, my_collate, merge_sample_lists\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n",
    "\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "# Use mixup in SSL training, Use UDA maybe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:47:17.868322Z",
     "start_time": "2020-10-10T05:47:17.734972Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_preprocess_text():\n",
    "    char_level = {\"keyboard\": 0.1, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1}\n",
    "    char_level = TextAugment([0.1, 0.4, 0.5], char_level)\n",
    "    word_level = {\"split\": 0.2,\n",
    "                 \"stopword_insert\": 0.0, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    word_level = TextAugment([0.1, 0.4, 0.5], word_level, \n",
    "                             fasttext_file=\"wiki-news-300d-1M-subword.bin\")\n",
    "    sentence_level = {\"text_rotate\": 0.0, \"sentence_shuffle\": 0.0, # \"glove_twitter\": 0.75,\"word_cutout\": 0.5,\n",
    "                      \"one_third_cut\": 0.25, \"half_cut\":0.0, \"part_select\": 0.25, }\n",
    "    sentence_level = TextAugment([0.1, 0.9], sentence_level, # idf_file=\"/home/ahemf/cache/tfidf_terms.csv\"\n",
    "                                )\n",
    "    gibberish = {\"gibberish_insert\": 0.25, \"punctuation_insert\": 0.75, \n",
    "                 \"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5,}\n",
    "    gibberish = TextAugment([0.25, 0.75], gibberish)\n",
    "    # translation = {\"dab\":1.0, \"punctuation_insert\": 0.01}\n",
    "    # translation = TextAugment([0.0, 1.0], translation, dab_file=\"/home/ahemf/cache/fdab.csv\")\n",
    "    def process(text, **kwargs):\n",
    "        if random.random() < 0.25:\n",
    "            text = sentence_level(text, **kwargs)\n",
    "        # else:\n",
    "            # text = translation(text, **kwargs)\n",
    "        text = word_level(text, **kwargs)\n",
    "        text = char_level(text, **kwargs)\n",
    "        text = gibberish(text, **kwargs)\n",
    "        return text\n",
    "    return process\n",
    "\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "transforms_for_bbox_methods = get_transforms_for_bbox_methods()\n",
    "\n",
    "vectorized_text_processor = np.vectorize(preprocess_text)\n",
    "def vectorized_image_processor(images):\n",
    "    return [transforms_for_bbox_methods(i) for i in images]\n",
    "\n",
    "def augment_method(sampleList):\n",
    "    sampleList = dict2sampleList(sampleList, device=get_device())\n",
    "    sampleList = sampleList.copy()\n",
    "    sampleList.image = vectorized_image_processor(sampleList.original_image)\n",
    "    sampleList.text = vectorized_text_processor(sampleList.original_text)\n",
    "    sampleList.mixup = [False] * len(sampleList.text)\n",
    "    sampleList = sampleList.to(get_device())\n",
    "    return sampleList\n",
    "\n",
    "\n",
    "def get_view_1():\n",
    "    augs = {\"keyboard\": 0.4, \"char_substitute\": 0.4, \"char_insert\": 0.2, \"char_swap\": 0.2, \n",
    "                  \"ocr\": 0.0, \"char_delete\": 0.1, \"gibberish_insert\": 0.25, \"punctuation_insert\": 0.75, \n",
    "                 \"punctuation_replace\": 0.25, \"punctuation_strip\": 0.5, \"word_join\": 0.2, \"punctuation_continue\": 0.5}\n",
    "    text_augs = TextAugment([0.1, 0.9,], augs)\n",
    "    imtrans = transforms.RandomHorizontalFlip(p=1.0)\n",
    "    vtp = np.vectorize(text_augs)\n",
    "    def vip(images):\n",
    "        return [imtrans(i) for i in images]\n",
    "\n",
    "    def aug(sampleList):\n",
    "        sampleList = dict2sampleList(sampleList, device=get_device())\n",
    "        sampleList = sampleList.copy()\n",
    "        sampleList.image = vip(sampleList.image)\n",
    "        sampleList.text = vtp(sampleList.original_text)\n",
    "        sampleList.mixup = [False] * len(sampleList.text)\n",
    "        sampleList = sampleList.to(get_device())\n",
    "        return sampleList\n",
    "    return aug\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\",\n",
    "                    train_text_transform=None,\n",
    "                    train_image_transform=None,\n",
    "                    test_text_transform=None,\n",
    "                    test_image_transform=None,\n",
    "                    train_torchvision_pre_image_transform=None,\n",
    "                    test_torchvision_pre_image_transform=None,\n",
    "                    cache_images=False,\n",
    "                    use_images=True,\n",
    "                    dev=False,\n",
    "                    test_dev=True,\n",
    "                    keep_original_text=True,\n",
    "                    keep_original_image=True,\n",
    "                    keep_processed_image=False,\n",
    "                    keep_torchvision_image=False,\n",
    "                    train_mixup_config=None)\n",
    "\n",
    "\n",
    "data[\"test\"][\"label\"] = -1\n",
    "\n",
    "df = pd.concat((data[\"train\"],\n",
    "                data[\"dev\"], \n",
    "                data[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:47:17.877816Z",
     "start_time": "2020-10-10T05:47:17.870476Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = convert_dataframe_to_dataset(df, data[\"metadata\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:48:04.026636Z",
     "start_time": "2020-10-10T05:47:17.879831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "Overriding option model to vilbert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "Overriding option model to visual_bert\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n",
      "Overriding option config to projects/hateful_memes/configs/mmbt/with_features.yaml\n",
      "Overriding option model to mmbt\n",
      "Overriding option datasets to hateful_memes\n",
      "Overriding option run_type to val\n",
      "Overriding option checkpoint.resume_zoo to mmbt.hateful_memes.features\n",
      "Overriding option evaluation.predict to true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n",
      "Config '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/utils/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml' has no VERSION. Assuming it to be compatible with latest v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N tokens Out =  164 Classifier Dims =  768 Matches embedding_dims:  True\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(\n",
    "    model_name={\"lxmert\": dict(dropout=0.05, gaussian_noise=0.01), \n",
    "                \"vilbert\": dict(dropout=0.1, gaussian_noise=0.05), \n",
    "                \"visual_bert\": dict(dropout=0.1, gaussian_noise=0.05), \n",
    "                \"mmbt_region\": dict(dropout=0.1, gaussian_noise=0.05)},\n",
    "    num_classes=2,\n",
    "    gaussian_noise=0.0,\n",
    "    dropout=0.0,\n",
    "    word_masking_proba=0.125,\n",
    "    featurizer=\"pass\",\n",
    "    final_layer_builder=fb_1d_loss_builder,\n",
    "    internal_dims=768,\n",
    "    classifier_dims=768,\n",
    "    n_tokens_in=128,\n",
    "    n_tokens_out=128,\n",
    "    n_layers=0,\n",
    "    attention_drop_proba=0.0,\n",
    "    loss=\"focal\",\n",
    "    dice_loss_coef=0.0,\n",
    "    auc_loss_coef=0.0,\n",
    "    bbox_swaps=1,\n",
    "    bbox_copies=1,\n",
    "    bbox_deletes=0,\n",
    "    bbox_gaussian_noise=0.01,\n",
    "    view_transforms=[get_view_1()],\n",
    "    finetune=False)\n",
    "\n",
    "model_class = VilBertVisualBertModelV2\n",
    "model = model_class(**model_params)\n",
    "model = model.to(get_device())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unimodal MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:48:04.748592Z",
     "start_time": "2020-10-10T05:48:04.028309Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=1e-4, weight_decay=1e-2)\n",
    "\n",
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBertV2 import positive, negative\n",
    "mlm_model = MLMSimCLR(model, 0.1, {1: negative, 0: positive}, augment_method, augment_method)\n",
    "mlm_model = mlm_model.to(get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-10T05:47:17.269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autocast =  True Epochs =  1 Divisor = 1 Examples = 10000 Batch Size =  3\n",
      "Training Samples =  10000 Weighted Sampling =  False Num Batches =  3334 Accumulation steps =  4\n",
      "[WARN]: Number of training batches not divisible by accumulation steps, some training batches will be wasted due to this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae941a963314fa78e5929102b86ea83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962edda6bac34d24a4f622589faf7cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=3334.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureExtractor : Loaded Model...\n",
      "Modifications for VG in RPN (modeling/proposal_generator/rpn.py):\n",
      "\tUse hidden dim 512 instead fo the same dim as Res4 (1024).\n",
      "\n",
      "Modifications for VG in RoI heads (modeling/roi_heads/fast_rcnn.py))\n",
      "\tEmbedding: 1601 --> 256\tLinear: 2304 --> 512\tLinear: 512 --> 401\n",
      "\n",
      "LXMERTFeatureExtractor : Loaded Model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"vilbert\": {\"finetune\": False,},\n",
    "        \"visual_bert\": {\"finetune\": False,},\n",
    "        \"mmbt_region\": {\"finetune\": False,},\n",
    "        \"lxmert\": {\"finetune\": False,},\n",
    "    },\n",
    "    \"mlms\": {\"finetune\": True},\n",
    "    \"simclr_layer\": {\"finetune\": True},\n",
    "}\n",
    "epochs = 1\n",
    "batch_size = 3\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "\n",
    "_ = group_wise_finetune(mlm_model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(mlm_model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "train_losses, learning_rates, _ = train(mlm_model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=4, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "\n",
    "\n",
    "mlm_model.plot_loss_acc_hist()\n",
    "mlm_model.test_accuracy(batch_size, dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T11:24:41.962482Z",
     "start_time": "2020-08-09T11:24:41.960247Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(mlm_model.state_dict(), \"lxmert-mlm-init.pth\")\n",
    "# mlm_model.load_state_dict(torch.load(\"lxmert-mlm-init.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T17:58:06.163460Z",
     "start_time": "2020-08-09T15:05:22.244073Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "epochs = 5\n",
    "batch_size = 48\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-4)\n",
    "\n",
    "lr_strategy = {\n",
    "    \"model\": {\n",
    "        \"finetune\": True,\n",
    "        \"lr\": optimizer_params[\"lr\"]\n",
    "    },\n",
    "    \"mlm\": {\n",
    "        \"finetune\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(mlm_model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(mlm_model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "train_losses, learning_rates, _ = train(mlm_model, optimizer, scheduler_init_fn, batch_size, epochs, dataset,\n",
    "                                     model_call_back=None, accumulation_steps=5, plot=True,\n",
    "                                     sampling_policy=None, class_weights=None)\n",
    "\n",
    "\n",
    "\n",
    "mlm_model.plot_loss_acc_hist()\n",
    "acc = mlm_model.test_accuracy(batch_size, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T04:45:15.996131Z",
     "start_time": "2020-07-31T04:14:22.512090Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = mlm_model.test_accuracy(batch_size, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T19:12:59.352087Z",
     "start_time": "2020-08-09T19:12:51.954015Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(mlm_model.model.state_dict(), \"lxmert-mlm.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# AugSim\n",
    "- Combine both unimodal and bimodal augsim using `random.random`\n",
    "- Take hints from SimCLR\n",
    "- We can do Text x Image (TODO: CrissCrossDataset for Augsim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T19:18:06.984384Z",
     "start_time": "2020-08-09T19:18:06.364731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_stored_params(model, \"lxmert-mlm.pth\")\n",
    "set_global(\"cache_allow_writes\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-09T19:18:22.043392Z",
     "start_time": "2020-08-09T19:18:22.039762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-3)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T05:01:29.487245Z",
     "start_time": "2020-08-09T19:18:35.086830Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy_model)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy_model)\n",
    "optim = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "_ = train_for_augment_similarity(model,\n",
    "                                 optim,\n",
    "                                 scheduler_init_fn,\n",
    "                                 batch_size,\n",
    "                                 epochs,\n",
    "                                 dataset,\n",
    "                                 augment_method=augment_method,\n",
    "                                 model_call_back=None,\n",
    "                                 collate_fn=my_collate,\n",
    "                                 accumulation_steps=4,\n",
    "                                 plot=True)\n",
    "# 0.001580, 0.000527\n",
    "# Try Augsim with L2 normed / LayerNormed vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T06:34:49.734072Z",
     "start_time": "2020-08-10T06:34:47.808602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lxmert-augsim.pth\")\n",
    "# model.load_state_dict(torch.load(\"lxmert-augsim.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimCLR style or Differentiator\n",
    "- Combine Unimodal and Bimodal with probability\n",
    "- In unimodal differentiator we only change either text or image\n",
    "- Ability to use non-overlapping image sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T20:04:06.453139Z",
     "start_time": "2020-08-14T20:04:05.321942Z"
    }
   },
   "outputs": [],
   "source": [
    "load_stored_params(model, \"lxmert-smclr.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T20:04:06.459375Z",
     "start_time": "2020-08-14T20:04:06.455212Z"
    }
   },
   "outputs": [],
   "source": [
    "from facebook_hateful_memes_detector.utils import SimCLR\n",
    "\n",
    "def simclr_aug(sampleList):\n",
    "    sampleList = augment_method(sampleList.copy())\n",
    "    s2 = sampleList.copy()\n",
    "    s2.text = list(reversed(s2.text))\n",
    "    s = merge_sample_lists(sampleList, s2)\n",
    "    return s\n",
    "\n",
    "# set_global(\"cache_allow_writes\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T20:04:06.477539Z",
     "start_time": "2020-08-14T20:04:06.461220Z"
    }
   },
   "outputs": [],
   "source": [
    "smclr = SimCLR(model, 768, 256, 0.05, simclr_aug, simclr_aug)\n",
    "smclr = smclr.to(get_device())\n",
    "\n",
    "lr_strategy_pre = {\n",
    "    \"finetune\": True,\n",
    "    \"model\": {\n",
    "        \"finetune\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "lr_strategy_post = {\n",
    "    \"finetune\": True,\n",
    "}\n",
    "\n",
    "pre_lr, post_lr = 5e-5, 5e-5\n",
    "pre_batch_size, post_batch_size = 256, 32\n",
    "pre_epochs, full_epochs = 2, 5\n",
    "collate_fn = my_collate\n",
    "\n",
    "def simclr_aug(sampleList):\n",
    "    sampleList = augment_method(sampleList.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-14T20:02:10.067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = run_simclr(smclr, dataset, dataset, lr_strategy_pre, lr_strategy_post, pre_lr, post_lr,\n",
    "           pre_batch_size, post_batch_size, pre_epochs, full_epochs,\n",
    "           collate_fn)\n",
    "\n",
    "res\n",
    "\n",
    "# 0.3268\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-14T20:00:29.595925Z",
     "start_time": "2020-08-14T20:00:22.646123Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lxmert-smclr.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

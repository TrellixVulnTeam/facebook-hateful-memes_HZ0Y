{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:09.007836Z",
     "start_time": "2020-08-12T09:39:05.799518Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch_optimizer as optim\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.options.display.width = 0\n",
    "import warnings\n",
    "import torchvision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from facebook_hateful_memes_detector.utils.globals import set_global, get_global\n",
    "set_global(\"cache_dir\", \"/home/ahemf/cache/cache\")\n",
    "set_global(\"dataloader_workers\", 4)\n",
    "set_global(\"use_autocast\", True)\n",
    "set_global(\"models_dir\", \"/home/ahemf/cache/\")\n",
    "\n",
    "from facebook_hateful_memes_detector.utils import read_json_lines_into_df, in_notebook, set_device, print_code, my_collate, load_stored_params\n",
    "get_global(\"cache_dir\")\n",
    "from facebook_hateful_memes_detector.models import Fasttext1DCNNModel, MultiImageMultiTextAttentionEarlyFusionModel, VilBertVisualBertModel\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset, get_datasets, get_image2torchvision_transforms, TextAugment\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedRotation, QuadrantCut, ImageAugment, DefinedAffine, DefinedColorJitter, DefinedRandomPerspective\n",
    "from facebook_hateful_memes_detector.preprocessing import DefinedAffine, HalfSwap, get_image_transforms, get_transforms_for_bbox_methods\n",
    "from facebook_hateful_memes_detector.training import *\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:09.121045Z",
     "start_time": "2020-08-12T09:39:09.009458Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_preprocess_text():\n",
    "    char_level = {\n",
    "        \"keyboard\": 0.1,\n",
    "        \"char_substitute\": 0.4,\n",
    "        \"char_insert\": 0.2,\n",
    "        \"char_swap\": 0.2,\n",
    "        \"ocr\": 0.0,\n",
    "        \"char_delete\": 0.1\n",
    "    }\n",
    "    char_level = TextAugment([0.1, 0.4, 0.5], char_level)\n",
    "    word_level = {\n",
    "        \"fasttext\": 0.0,\n",
    "        \"glove_twitter\": 0.0,\n",
    "        \"glove_wiki\": 0.0,\n",
    "        \"word2vec\": 0.0,\n",
    "        \"split\": 0.2,\n",
    "        \"stopword_insert\": 0.0,\n",
    "        \"word_join\": 0.2,\n",
    "        \"word_cutout\": 0.8,\n",
    "        \"gibberish_insert\": 0.0\n",
    "    }\n",
    "    word_level = TextAugment([0.1, 0.4, 0.5], word_level)\n",
    "    sentence_level = {\n",
    "        \"text_rotate\": 0.0,\n",
    "        \"sentence_shuffle\": 0.0,\n",
    "        \"one_third_cut\": 0.3,\n",
    "        \"half_cut\": 0.0,\n",
    "        \"part_select\": 0.75\n",
    "    }\n",
    "    sentence_level = TextAugment([0.75, 0.25], sentence_level)\n",
    "    gibberish = {\n",
    "        \"gibberish_insert\": 0.25,\n",
    "        \"punctuation_insert\": 0.75,\n",
    "    }\n",
    "    gibberish = TextAugment([0.75, 0.25], gibberish)\n",
    "\n",
    "    def process(text):\n",
    "        text = sentence_level(text)\n",
    "        text = word_level(text)\n",
    "        text = char_level(text)\n",
    "        text = gibberish(text)\n",
    "        return text\n",
    "\n",
    "    return process\n",
    "\n",
    "\n",
    "preprocess_text = get_preprocess_text()\n",
    "transforms_for_bbox_methods = get_transforms_for_bbox_methods()\n",
    "\n",
    "\n",
    "data = get_datasets(data_dir=\"../data/\", train_text_transform=preprocess_text, train_image_transform=transforms_for_bbox_methods, \n",
    "                    test_text_transform=None, test_image_transform=None, \n",
    "                    cache_images = True, use_images = True, dev=False, test_dev=True,\n",
    "                    keep_original_text=False, keep_original_image=False, \n",
    "                    keep_processed_image=True, keep_torchvision_image=False,)\n",
    "\n",
    "# ImageAugment([0.2, 0.5, 0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:10.384747Z",
     "start_time": "2020-08-12T09:39:10.375277Z"
    }
   },
   "outputs": [],
   "source": [
    "sgd = torch.optim.SGD\n",
    "sgd_params = dict(lr=2e-2,\n",
    "                  momentum=0.9,\n",
    "                  dampening=0,\n",
    "                  weight_decay=0,\n",
    "                  nesterov=False)\n",
    "\n",
    "rangerQH = optim.RangerQH\n",
    "rangerQHparams = dict(\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    nus=(.7, 1.0),\n",
    "    weight_decay=0.0,\n",
    "    k=6,\n",
    "    alpha=.5,\n",
    "    decouple_weight_decay=True,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "adam = torch.optim.Adam\n",
    "adam_params = params = dict(lr=1e-3, weight_decay=1e-7)\n",
    "\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-2)\n",
    "\n",
    "novograd = optim.NovoGrad\n",
    "novograd_params = dict(\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    grad_averaging=False,\n",
    "    amsgrad=False,\n",
    ")\n",
    "\n",
    "qhadam = optim.QHAdam\n",
    "qhadam_params = dict(\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    nus=(1.0, 1.0),\n",
    "    weight_decay=0,\n",
    "    decouple_weight_decay=False,\n",
    "    eps=1e-8,\n",
    ")\n",
    "\n",
    "radam = optim.RAdam\n",
    "radam_params = dict(\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    ")\n",
    "\n",
    "yogi = optim.Yogi\n",
    "yogi_params = dict(lr=1e-2,\n",
    "                   betas=(0.9, 0.999),\n",
    "                   eps=1e-3,\n",
    "                   initial_accumulator=1e-6,\n",
    "                   weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:11.088255Z",
     "start_time": "2020-08-12T09:39:11.084275Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=96\n",
    "epochs = 10\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=1e-4, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "\n",
    "scheduler_init_fn = get_cosine_schedule_with_warmup()\n",
    "reg_sched = get_regularizer_scheduler()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:12.274468Z",
     "start_time": "2020-08-12T09:39:12.270196Z"
    }
   },
   "outputs": [],
   "source": [
    "lxmert_strategy = {\n",
    "    \"lxmert\": {\n",
    "        \"model\": {\n",
    "            \"bert\": {\n",
    "                \"encoder\": {\n",
    "                    \"x_layers\": {\n",
    "                        \"lr\": optimizer_params[\"lr\"],\n",
    "                        \"finetune\": True\n",
    "                    },\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": False\n",
    "                },\n",
    "                \"pooler\": {\n",
    "                    \"lr\": optimizer_params[\"lr\"],\n",
    "                    \"finetune\": True\n",
    "                },\n",
    "            },\n",
    "            \"finetune\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "lxmert_strategy = {\n",
    "    \"finetune\": True\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LXMERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:39:15.868307Z",
     "start_time": "2020-08-12T09:39:15.862729Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params = dict(\n",
    "    model_name={\"lxmert\": dict(dropout=0.2, gaussian_noise=0.01)},\n",
    "    num_classes=2,\n",
    "    gaussian_noise=0.01,\n",
    "    dropout=0.2,\n",
    "    word_masking_proba=0.2,\n",
    "    featurizer=\"pass\",\n",
    "    final_layer_builder=fb_1d_loss_builder,\n",
    "    internal_dims=768,\n",
    "    classifier_dims=768,\n",
    "    n_tokens_in=96,\n",
    "    n_tokens_out=96,\n",
    "    n_layers=0,\n",
    "    attention_drop_proba=0.0,\n",
    "    loss=\"focal\",\n",
    "    dice_loss_coef=0.0,\n",
    "    auc_loss_coef=0.0,\n",
    "    bbox_swaps=5,\n",
    "    bbox_copies=5,\n",
    "    bbox_gaussian_noise=0.05,\n",
    "    finetune=False)\n",
    "\n",
    "from facebook_hateful_memes_detector.models.MultiModal.VilBertVisualBert import VilBertVisualBertModel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_fn = model_builder(VilBertVisualBertModel,\n",
    "                         model_params,\n",
    "                         per_param_opts_fn=lxmert_strategy,\n",
    "                         optimiser_class=optimizer_class,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T09:40:31.713753Z",
     "start_time": "2020-08-12T09:39:20.137299Z"
    }
   },
   "outputs": [],
   "source": [
    "model, optimizer = model_fn()\n",
    "# load_stored_params(model, \"lxmert-smclr.pth\")\n",
    "model = model.to(get_device())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T10:23:38.904430Z",
     "start_time": "2020-08-12T09:40:31.715644Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_strategy = {\n",
    "    \"finetune\": False,\n",
    "    \"model_heads\": {\n",
    "        \"finetune\": True,\n",
    "    },\n",
    "    \"final_layer\": {\n",
    "        \"finetune\": True,\n",
    "    },\n",
    "}\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "batch_size=128\n",
    "epochs = 6\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched,\n",
    "    validation_epochs=[2, 5, 7, 9, 11, 14, 17, 19, 24, 28],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    accumulation_steps=2,\n",
    ")\n",
    "r1, p1 = results, prfs\n",
    "results\n",
    "prfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-12T12:49:38.080035Z",
     "start_time": "2020-08-12T10:23:38.906861Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_strategy = {\n",
    "    \"finetune\": True\n",
    "}\n",
    "adamw = torch.optim.AdamW\n",
    "adamw_params = dict(lr=5e-5, betas=(0.9, 0.98), eps=1e-08, weight_decay=1e-2)\n",
    "optimizer_class = adamw\n",
    "optimizer_params = adamw_params\n",
    "\n",
    "_ = group_wise_finetune(model, lr_strategy)\n",
    "params_conf, _ = group_wise_lr(model, lr_strategy)\n",
    "optimizer = optimizer_class(params_conf, **optimizer_params)\n",
    "\n",
    "batch_size=64\n",
    "epochs = 16\n",
    "\n",
    "kfold = False\n",
    "results, prfs = train_validate_ntimes(\n",
    "    (model, optimizer),\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    kfold=kfold,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    model_call_back=reg_sched,\n",
    "    validation_epochs=[5, 7, 9, 11, 15, 17, 24, 28, 31],\n",
    "    show_model_stats=False,\n",
    "    sampling_policy=\"without_replacement\",\n",
    "    accumulation_steps=4,\n",
    ")\n",
    "r1, p1 = results, prfs\n",
    "results\n",
    "prfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-12T19:25:28.522Z"
    }
   },
   "outputs": [],
   "source": [
    "preds, probas = [], []\n",
    "dataset = convert_dataframe_to_dataset(data[\"dev\"], data[\"metadata\"], True)\n",
    "for i in range(5):\n",
    "    proba_list, all_probas_list, predictions_list, labels_list = generate_predictions(model, 128, dataset)\n",
    "    probas.append(all_probas_list)\n",
    "    preds.append(predictions_list)\n",
    "    \n",
    "from collections import Counter\n",
    "preds_voted = [Counter(p).most_common()[0][0] for p in zip(*preds)]\n",
    "probas_mean = torch.tensor(probas).mean(0)\n",
    "pred_probas = probas_mean.max(dim=1).indices\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "\n",
    "print(accuracy_score(labels_list, preds_voted))\n",
    "print(accuracy_score(labels_list, pred_probas))\n",
    "print(roc_auc_score(labels_list, probas_mean[:, 1].tolist(), multi_class=\"ovo\", average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-19T17:29:34.088204Z",
     "start_time": "2020-07-19T17:29:34.080393Z"
    }
   },
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam\n",
    "adam_params = params = dict(lr=1e-4, weight_decay=1e-2)\n",
    "optimizer = adam\n",
    "optimizer_params = adam_params\n",
    "\n",
    "model_fn = model_builder(VilBertVisualBertModel,\n",
    "                         dict(model_name={\n",
    "                             \"lxmert\":\n",
    "                             dict(finetune=True,\n",
    "                                  dropout=0.1,\n",
    "                                  gaussian_noise=0.2),\n",
    "                            },\n",
    "                              num_classes=2,\n",
    "                              gaussian_noise=0.2,\n",
    "                              dropout=0.25,\n",
    "                              featurizer=\"pass\",\n",
    "                              final_layer_builder=fb_1d_loss_builder,\n",
    "                              internal_dims=768,\n",
    "                              classifier_dims=768,\n",
    "                              n_tokens_in=96,\n",
    "                              n_tokens_out=96,\n",
    "                              n_layers=2,\n",
    "                              loss=\"focal\",\n",
    "                              dice_loss_coef=0.0,\n",
    "                              auc_loss_coef=0.0,\n",
    "                              word_masking_proba=0.2),\n",
    "                         per_param_opts_fn=combo_strategy,\n",
    "                         optimiser_class=optimizer,\n",
    "                         optimiser_params=optimizer_params)\n",
    "\n",
    "# model, opt = model_fn()\n",
    "# model\n",
    "\n",
    "##\n",
    "## MMBT Region, Per module regularization, word_masking_proba, reg_scheduling\n",
    "\n",
    "## Next accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T20:30:33.609035Z",
     "start_time": "2020-07-19T17:29:37.174068Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "batch_size = 4\n",
    "epochs = 7\n",
    "\n",
    "submission, text_model = train_and_predict(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    accumulation_steps=16,\n",
    "    model_call_back=reg_sched,\n",
    "    sampling_policy=\"without_replacement\") # \"without_replacement\"\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T20:39:54.691953Z",
     "start_time": "2020-07-20T20:30:33.610859Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"test\"] = data[\"dev\"]\n",
    "sf, _ = predict(text_model, data, batch_size)\n",
    "\n",
    "print(sf.head())\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "labels_list = data[\"test\"].label\n",
    "proba_list = sf.proba\n",
    "predictions_list = sf.label\n",
    "\n",
    "auc = roc_auc_score(labels_list, proba_list)\n",
    "# p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"micro\")\n",
    "prfs = precision_recall_fscore_support(labels_list, predictions_list, average=None, labels=[0, 1])\n",
    "map = average_precision_score(labels_list, proba_list)\n",
    "acc = accuracy_score(labels_list, predictions_list)\n",
    "validation_scores = [map, acc, auc]\n",
    "print(\"scores = \", dict(zip([\"map\", \"acc\", \"auc\"], [\"%.4f\" % v for v in validation_scores])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-18T18:45:52.939712Z",
     "start_time": "2020-07-18T18:42:25.911Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.sample(10)\n",
    "submission.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-16T01:18:47.528516Z",
     "start_time": "2020-07-15T21:42:09.784956Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 7\n",
    "\n",
    "submission, text_model = train_and_predict(\n",
    "    model_fn,\n",
    "    data,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    scheduler_init_fn=scheduler_init_fn,\n",
    "    accumulation_steps=1,\n",
    "    model_call_back=reg_sched,\n",
    "    sampling_policy=None) # \"without_replacement\"\n",
    "\n",
    "submission.to_csv(\"submission2.csv\", index=False)\n",
    "submission.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

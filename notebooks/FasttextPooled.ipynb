{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T08:03:48.380550Z",
     "start_time": "2020-05-24T07:55:20.573496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-24 07:55:20--  https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 2606:4700:10::6816:4b8e, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5828358084 (5.4G) [application/zip]\n",
      "Saving to: ‘crawl-300d-2M-subword.zip’\n",
      "\n",
      "100%[====================================>] 5,828,358,084 13.4MB/s   in 7m 9s  \n",
      "\n",
      "2020-05-24 08:02:30 (12.9 MB/s) - ‘crawl-300d-2M-subword.zip’ saved [5828358084/5828358084]\n",
      "\n",
      "Archive:  crawl-300d-2M-subword.zip\n",
      "  inflating: crawl-300d-2M-subword.vec  \n",
      "  inflating: crawl-300d-2M-subword.bin  \n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\n",
    "!unzip crawl-300d-2M-subword.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T18:17:52.642557Z",
     "start_time": "2020-05-24T18:17:52.639753Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "sys.path.append(os.path.abspath(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T18:19:16.520438Z",
     "start_time": "2020-05-24T18:19:16.503004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'facebook_hateful_memes_detector' from '/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/__init__.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from importlib import reload\n",
    "import facebook_hateful_memes_detector\n",
    "reload(facebook_hateful_memes_detector)\n",
    "\n",
    "from facebook_hateful_memes_detector import read_json_lines_into_df\n",
    "from facebook_hateful_memes_detector import FasttextPooledModel\n",
    "from facebook_hateful_memes_detector.preprocessing import TextImageDataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T18:19:16.791151Z",
     "start_time": "2020-05-24T18:19:16.788582Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T18:19:18.596501Z",
     "start_time": "2020-05-24T18:19:18.445913Z"
    }
   },
   "outputs": [],
   "source": [
    "dev = read_json_lines_into_df('../data/dev.jsonl')\n",
    "train = read_json_lines_into_df('../data/train.jsonl')\n",
    "test = read_json_lines_into_df('../data/test.jsonl')\n",
    "\n",
    "train_text = list(train.text)\n",
    "dev_text = list(dev.text)\n",
    "test_text = list(test.text)\n",
    "\n",
    "train_img = list(map(lambda l: \"../data/\"+l,train.img))\n",
    "test_img = list(map(lambda l: \"../data/\"+l,test.img))\n",
    "dev_img = list(map(lambda l: \"../data/\"+l,dev.img))\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(categories=[[0, 1]],handle_unknown='error', sparse=False)\n",
    "train_labels = torch.tensor(train.label)\n",
    "dev_labels = torch.tensor(dev.label)\n",
    "\n",
    "train_text = np.array(train_text)\n",
    "dev_text = np.array(dev_text)\n",
    "test_text = np.array(test_text)\n",
    "\n",
    "train_img = np.array(train_img)\n",
    "test_img = np.array(test_img)\n",
    "dev_img = np.array(dev_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T18:19:19.349170Z",
     "start_time": "2020-05-24T18:19:19.343966Z"
    }
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "def my_collate(batch):\n",
    "    text = [item[0] for item in batch]\n",
    "    image = torch.stack([item[1] for item in batch])\n",
    "    label = [item[2] for item in batch]\n",
    "    label = torch.LongTensor(label) if label[0] is not None else label\n",
    "    return [text, image, label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:49:33.858728Z",
     "start_time": "2020-05-24T15:49:30.411947Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "channel3 = [np.array(Image.open(im)).shape[2] == 3 for im in train_img]\n",
    "all(channel3)\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm, trange\n",
    "training_fold_dataset = TextImageDataset(train_text, train_img, train_labels, image_transform=preprocess)\n",
    "testing_fold_dataset = TextImageDataset(test_text, test_img, image_transform=preprocess)\n",
    "train_loader = DataLoader(training_fold_dataset, batch_size=64, collate_fn=my_collate, \n",
    "                          shuffle=False,num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start Iterating\")\n",
    "epochs = 10\n",
    "for epoch in tqdm_notebook(range(epochs)):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    accuracy = 0\n",
    "    print(\"Epoch = \", epoch)\n",
    "\n",
    "    # Training the model\n",
    "    counter = 0\n",
    "    for i, (texts, images, labels) in enumerate(tqdm_notebook(train_loader)):\n",
    "        print(\"Epoch = \", epoch, \"Sub = \", i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-24T19:28:30.962Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm_notebook as tqdm, trange\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "n_tests = 1\n",
    "for _ in range(n_tests):\n",
    "    dataset = TextImageDataset(train_text, train_img, train_labels, image_transform=preprocess)\n",
    "    size = len(dataset)\n",
    "    training_fold_dataset, testing_fold_dataset = torch.utils.data.random_split(dataset, [int(size*0.8), size - int(size*0.8)])\n",
    "    \n",
    "    train_loader = DataLoader(training_fold_dataset, batch_size=512, collate_fn=my_collate, \n",
    "                              shuffle=True,num_workers=32, pin_memory=True)\n",
    "    val_loader = DataLoader(testing_fold_dataset, batch_size=512, collate_fn=my_collate, \n",
    "                            shuffle=True,num_workers=32, pin_memory=True)\n",
    "    \n",
    "    model = FasttextPooledModel(300, 1024, 2, \"crawl-300d-2M-subword.bin\",)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    epochs = 5\n",
    "    try:\n",
    "        with tqdm(list(range(epochs))) as epo:\n",
    "            for epoch in epo:\n",
    "                train_loss = 0\n",
    "                val_loss = 0\n",
    "                accuracy = 0\n",
    "\n",
    "                # Training the model\n",
    "                _ = model.train()\n",
    "                counter = 0\n",
    "                with tqdm(train_loader) as data_batch:\n",
    "                    for texts, images, labels in data_batch:\n",
    "                        optimizer.zero_grad()\n",
    "                        logits, loss = model.forward(texts, images, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        train_loss += loss.item()*labels.size(0)\n",
    "\n",
    "\n",
    "    except (KeyboardInterrupt, Exception) as e:\n",
    "        epochs.close()\n",
    "        data_batch.close()\n",
    "        raise\n",
    "    epo.close()\n",
    "    data_batch.close()\n",
    "    \n",
    "    _ = model.eval()\n",
    "    labels_list = []\n",
    "    proba_list = []\n",
    "    predictions_list = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for texts, images, labels in val_loader:\n",
    "            logits, valloss = model.forward(texts, images, labels)\n",
    "            logits = torch.softmax(logits, dim=1)\n",
    "            top_p, top_class = logits.topk(1, dim=1)\n",
    "            labels = list(map(lambda x: x.item() ,list(labels)))\n",
    "            labels_list.extend(labels)\n",
    "            top_class = top_class.flatten()\n",
    "            top_class = list(map(lambda x: x.item() ,list(top_class)))\n",
    "            probas = list(map(lambda x: x.item() ,list(logits[:,1])))\n",
    "            predictions_list.extend(list(top_class))\n",
    "            proba_list.extend(probas)\n",
    "    auc = roc_auc_score(labels_list, proba_list)\n",
    "    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"micro\")\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"macro\")\n",
    "    p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"weighted\")\n",
    "    validation_scores = [p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, p_weighted, r_weighted, f1_weighted, auc]\n",
    "    \n",
    "    #\n",
    "    labels_list = []\n",
    "    proba_list = []\n",
    "    predictions_list = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for texts, images, labels in train_loader:\n",
    "            logits, valloss = model.forward(texts, images, labels)\n",
    "            logits = torch.softmax(logits, dim=1)\n",
    "            top_p, top_class = logits.topk(1, dim=1)\n",
    "            labels = list(map(lambda x: x.item() ,list(labels)))\n",
    "            labels_list.extend(labels)\n",
    "            top_class = top_class.flatten()\n",
    "            top_class = list(map(lambda x: x.item() ,list(top_class)))\n",
    "            probas = list(map(lambda x: x.item() ,list(logits[:,1])))\n",
    "            predictions_list.extend(list(top_class))\n",
    "            proba_list.extend(probas)\n",
    "    auc = roc_auc_score(labels_list, proba_list)\n",
    "    p_micro, r_micro, f1_micro, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"micro\")\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"macro\")\n",
    "    p_weighted, r_weighted, f1_weighted, _ = precision_recall_fscore_support(labels_list, predictions_list, average=\"weighted\")\n",
    "    train_scores = [p_micro, r_micro, f1_micro, p_macro, r_macro, f1_macro, p_weighted, r_weighted, f1_weighted, auc]\n",
    "    index = [\"p_micro\", \"r_micro\", \"f1_micro\", \"p_macro\", \"r_macro\", \"f1_macro\", \"p_weighted\", \"r_weighted\", \"f1_weighted\", \"auc\"]\n",
    "    pd.DataFrame(data=dict(train=train_scores, val=validation_scores), index=index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T19:28:26.122422Z",
     "start_time": "2020-05-24T19:27:54.100770Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/home/ahemf/mygit/facebook-hateful-memes/facebook_hateful_memes_detector/models/text_models/FasttextPooled.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TextImageDataset(test_text, test_img, None, image_transform=preprocess)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, collate_fn=my_collate, \n",
    "                              shuffle=False,num_workers=32, pin_memory=False)\n",
    "\n",
    "proba_list = []\n",
    "predictions_list = []\n",
    "with torch.no_grad():\n",
    "    for texts, images, labels in test_loader:\n",
    "        logits, valloss = model.forward(texts, images, labels)\n",
    "        logits = torch.softmax(logits, dim=1)\n",
    "        top_p, top_class = logits.topk(1, dim=1)\n",
    "        top_class = top_class.flatten()\n",
    "        top_class = list(map(lambda x: x.item() ,list(top_class)))\n",
    "        probas = list(map(lambda x: x.item() ,list(logits[:,1])))\n",
    "        predictions_list.extend(list(top_class))\n",
    "        proba_list.extend(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T19:28:26.136064Z",
     "start_time": "2020-05-24T19:28:26.124709Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(dict(id=test.id, proba=proba_list, label=predictions_list), columns=[\"id\", \"proba\", \"label\"])\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T19:28:26.144232Z",
     "start_time": "2020-05-24T19:28:26.137865Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proba</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16395</td>\n",
       "      <td>0.656999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37405</td>\n",
       "      <td>0.398898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94180</td>\n",
       "      <td>0.282765</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54321</td>\n",
       "      <td>0.467119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97015</td>\n",
       "      <td>0.589054</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     proba  label\n",
       "0  16395  0.656999      1\n",
       "1  37405  0.398898      0\n",
       "2  94180  0.282765      0\n",
       "3  54321  0.467119      0\n",
       "4  97015  0.589054      1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
